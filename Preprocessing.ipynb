{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Libraries"
      ],
      "metadata": {
        "id": "bdkP4uFzLq5t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYkvmYnl2Ydd"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from scipy.stats import mstats\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy import stats\n",
        "import random\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhouPsbhH1JA"
      },
      "outputs": [],
      "source": [
        "data=pd.read_csv('path_to_the_data')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NaN analysis"
      ],
      "metadata": {
        "id": "uMNq1qQlIOkH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XoruS9h8kUUw"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Columns with nans\n",
        "columnas_con_nans=data.columns[data.isna().any()].tolist()\n",
        "columnas_con_nans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4LzI1Bv8O49"
      },
      "outputs": [],
      "source": [
        "# Number of NaNs\n",
        "data.isna().sum().sum()>0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORMs210FFPAB"
      },
      "source": [
        "## Train-test division"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBg-lPs71QMx"
      },
      "outputs": [],
      "source": [
        "# To select 20% of the images for testing and 80% for training\n",
        "# Choose 88 numbers from 0 to 439\n",
        "\n",
        "random.seed(42)  # Set the random seed for reproducibility\n",
        "test_indices = random.sample(range(0, 440), 88)  # Generate a list of indices for testing\n",
        "# Complementary set for training\n",
        "train_indices = [i for i in range(0, 440) if i not in test_indices]  # Create a list of indices for training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJLZXnzrmOhL"
      },
      "outputs": [],
      "source": [
        "data_test=data[data['n_imagen'].isin(test_indices)]\n",
        "data_train=data[data['n_imagen'].isin(train_indices)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gdtIg5a5ZN-",
        "outputId": "855bac3a-ba92-4858-b682-5e9e444d7186"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(360448, 88)"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGydIKeDq_L6"
      },
      "outputs": [],
      "source": [
        "#Delete ID column\n",
        "data_train.drop(columns=['ID'], inplace=True)\n",
        "data_test.drop(columns=['ID'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0gkwLLO1ekN"
      },
      "outputs": [],
      "source": [
        "# Analysis of the target variable distribution\n",
        "target_variable = data['etiqueta']  # Extract the target variable\n",
        "features = data.drop(columns=['etiqueta', 'n_imagen', 'etiqueta_multi'])  # Remove unnecessary columns\n",
        "plt.hist(target_variable)  # Plot a histogram of the target variable\n",
        "plt.title('Distribution of Target Variable')  # Add a title to the histogram\n",
        "plt.xlabel('Value')  # Label the x-axis\n",
        "plt.ylabel('Frequency')  # Label the y-axis\n",
        "plt.show()  # Display the plot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIlA6nQKeIqA"
      },
      "source": [
        "## Outlier detection: One Class SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9jo1bY_eIT3"
      },
      "outputs": [],
      "source": [
        "data_train_outliers=data_train.drop(columns=['etiqueta', 'n_imagen', 'etiqueta_multi'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QHNJfQHeIIM"
      },
      "outputs": [],
      "source": [
        "# One-Class SVM for outlier detection\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Parameters\n",
        "nu = 0.01  # Fraction of outliers in the data\n",
        "random.seed(42)  # Set the random seed for reproducibility\n",
        "\n",
        "# Select relevant features (ensure you select numeric columns)\n",
        "features = data_train_outliers.columns\n",
        "df = data_train_outliers\n",
        "data = df[features]\n",
        "\n",
        "# Standardize the data (important for SVM)\n",
        "scaler = StandardScaler()\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "\n",
        "# Train the One-Class SVM model\n",
        "model = OneClassSVM(kernel='rbf', gamma='scale', nu=nu, verbose=True)\n",
        "# Adjust 'nu' based on the expected percentage of outliers\n",
        "model.fit(data_scaled)\n",
        "\n",
        "# Prediction (1: inliers, -1: outliers)\n",
        "df['outlier'] = model.predict(data_scaled)\n",
        "\n",
        "# Identify and save outliers\n",
        "outliers = df[df['outlier'] == -1]\n",
        "print(f\"Total outliers detected: {len(outliers)}\")\n",
        "outliers.to_csv('detected_outliers.csv', index=False)\n",
        "\n",
        "# Display some outliers\n",
        "print(outliers.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjHeKKqVq4nJ"
      },
      "outputs": [],
      "source": [
        "# Calculate the percentage of outliers\n",
        "percentage_outliers = (outliers.shape[0] / data_train.shape[0]) * 100\n",
        "print(f\"Percentage of outliers: {percentage_outliers:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czueGq2Vq4eW"
      },
      "outputs": [],
      "source": [
        "# Identify the indices of outliers\n",
        "outlier_indices = outliers.index\n",
        "\n",
        "# Filter out outliers from the training data\n",
        "data_train = data_train[~data_train.index.isin(outlier_indices)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AC-z9-g5DYU6"
      },
      "outputs": [],
      "source": [
        "data_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGsNzNNAoqih"
      },
      "source": [
        "## Correlation analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vv2MzWO0JOhW"
      },
      "outputs": [],
      "source": [
        "# Prepare training data\n",
        "features = data_train.drop(columns=['etiqueta', 'n_imagen', 'etiqueta_multi'])\n",
        "target = data_train['etiqueta']\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = features.corr()\n",
        "\n",
        "# Create a heatmap\n",
        "def plot_correlation_heatmap(matrix):\n",
        "    # Apply font settings in Matplotlib\n",
        "    plt.rcParams[\"font.family\"] = 'serif'\n",
        "    plt.figure(figsize=(16, 14))  # Adjust the figure size\n",
        "    sns.heatmap(\n",
        "        matrix,\n",
        "        annot=False,  # Hide numerical annotations\n",
        "        cmap=\"coolwarm\",  # Color palette\n",
        "        cbar=True,      # Show color bar\n",
        "        # Color bar range from -1 to 1\n",
        "        vmin=-1,\n",
        "        vmax=1,\n",
        "        center=0,\n",
        "        xticklabels=True,  # Show labels on the x-axis\n",
        "        yticklabels=True,  # Show labels on the y-axis\n",
        "        square=True      # Keep squares proportional\n",
        "    )\n",
        "    plt.show()\n",
        "\n",
        "# Usage\n",
        "plot_correlation_heatmap(correlation_matrix)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pyu6xNsUph-s"
      },
      "outputs": [],
      "source": [
        "# Set the correlation threshold\n",
        "threshold = 0.80\n",
        "\n",
        "# Function to identify highly correlated variable pairs\n",
        "def find_high_correlations(matrix, threshold):\n",
        "    # Select pairs of variables with correlation above the threshold\n",
        "    high_correlation = (matrix.where(matrix > threshold)\n",
        "                        .stack()\n",
        "                        .reset_index())\n",
        "\n",
        "    # Filter duplicates (since correlation is symmetric) and remove self-correlations\n",
        "    high_correlation = high_correlation[high_correlation['level_0'] != high_correlation['level_1']]\n",
        "\n",
        "    # Rename columns for clarity\n",
        "    high_correlation.columns = ['Variable1', 'Variable2', 'Correlation']\n",
        "\n",
        "    # Remove inverse duplicates (mirror), e.g., (Variable1, Variable2) and (Variable2, Variable1)\n",
        "    high_correlation = high_correlation.sort_values(by=['Variable1', 'Variable2']).drop_duplicates(subset=['Variable1', 'Variable2'])\n",
        "\n",
        "    return high_correlation\n",
        "\n",
        "# Usage\n",
        "high_correlation = find_high_correlations(correlation_matrix, threshold)\n",
        "\n",
        "# Display the result\n",
        "for index, row in high_correlation.iterrows():\n",
        "    print(f\"Variables: {row['Variable1']} and {row['Variable2']}, Correlation: {row['Correlation']:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhUqAg3cph-u"
      },
      "outputs": [],
      "source": [
        "# Initialize a list to store columns to remove\n",
        "columns_to_remove = []\n",
        "\n",
        "# Iterate over the correlation matrix\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i):\n",
        "        if correlation_matrix.iloc[i, j] > threshold:  # If correlation is above the threshold\n",
        "            colname = correlation_matrix.columns[i]   # Name of the highly correlated column\n",
        "            columns_to_remove.append(colname)            # Add to the list for removal\n",
        "\n",
        "# Remove duplicates from the list\n",
        "columns_to_remove = list(set(columns_to_remove))\n",
        "print(f\"Number of columns to remove: {len(columns_to_remove)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwV3ukzUp4W_"
      },
      "outputs": [],
      "source": [
        "# Remove columns\n",
        "data_train=data_train.drop(columns=columns_to_remove)\n",
        "data_test=data_test.drop(columns=columns_to_remove)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdJT9DNEu2yp"
      },
      "outputs": [],
      "source": [
        "# Identify constant columns\n",
        "constant_columns = []\n",
        "for column in data_train.columns:\n",
        "    if data_train[column].nunique() == 1:\n",
        "        constant_columns.append(column)\n",
        "print(constant_columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kZ-Hax3u9Qj"
      },
      "outputs": [],
      "source": [
        "# Remove constant columns\n",
        "data_train=data_train.drop(columns=columnas_constantes)\n",
        "data_test=data_test.drop(columns=columnas_constantes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPF_OJjVvzgV"
      },
      "outputs": [],
      "source": [
        "data_train.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIJkhPkruaM3"
      },
      "outputs": [],
      "source": [
        "data_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9s3VUwHka61"
      },
      "source": [
        "## Undersampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2j4lS0xklWxk"
      },
      "outputs": [],
      "source": [
        "# Count the number of samples with 'corrosion' in the 'etiqueta' column\n",
        "t_corrosion = data_train[data_train['etiqueta'] == 'corrosion'].shape[0]\n",
        "\n",
        "# Count the number of samples with 'no corrosion' in the 'etiqueta' column\n",
        "t_no_corrosion = data_train[data_train['etiqueta'] == 'no corrosion'].shape[0]\n",
        "\n",
        "# Calculate the difference to determine how many samples to remove\n",
        "data_to_remove = t_no_corrosion - t_corrosion\n",
        "\n",
        "print(f\"Data to remove: {data_to_remove}\")\n",
        "\n",
        "# Randomly remove data to balance classes\n",
        "random.seed(42)  # Set the random seed for reproducibility\n",
        "indices_to_remove = data_train[data_train['etiqueta'] == 'no corrosion'].sample(n=data_to_remove).index\n",
        "data_train = data_train.drop(indices_to_remove)\n",
        "print(data_train.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnxrtFwPnwkP"
      },
      "outputs": [],
      "source": [
        "# Verify class balance\n",
        "corrosion_count = data_train[data_train['etiqueta'] == 'corrosion'].shape[0]\n",
        "no_corrosion_count = data_train[data_train['etiqueta'] == 'no corrosion'].shape[0]\n",
        "print(f\"Corrosion count: {corrosion_count}, No corrosion count: {no_corrosion_count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Lp0BsOtoatC"
      },
      "source": [
        "## Homogeneity test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYd9WMF2Pfji"
      },
      "outputs": [],
      "source": [
        "# Function to classify features as homogeneous or non-homogeneous\n",
        "def classify_features(data, features, alpha=0.01):\n",
        "    homogeneous_features = []\n",
        "    non_homogeneous_features = []\n",
        "\n",
        "    for feature in features:\n",
        "        no_corrosion_values = data[data['etiqueta'] == 'no corrosion'][feature]\n",
        "        corrosion_values = data[data['etiqueta'] == 'corrosion'][feature]\n",
        "\n",
        "        stat, p_value = stats.mannwhitneyu(no_corrosion_values, corrosion_values)\n",
        "\n",
        "        if p_value < alpha:\n",
        "            non_homogeneous_features.append(feature)\n",
        "        else:\n",
        "            homogeneous_features.append(feature)\n",
        "\n",
        "    return homogeneous_features, non_homogeneous_features\n",
        "\n",
        "# Usage\n",
        "homogeneous_features, non_homogeneous_features = classify_features(data_train, X_train.columns)\n",
        "print(f\"Homogeneous features: {homogeneous_features}\")\n",
        "print(f\"Non-homogeneous features: {non_homogeneous_features}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKh1acJ2V7Ip"
      },
      "outputs": [],
      "source": [
        "# data_train.drop(columns=homogeneas, inplace=True)\n",
        "# data_test.drop(columns=homogeneas, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data analysis"
      ],
      "metadata": {
        "id": "r-lOn6NuPuGN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENlpL5p2jaV7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Improve style with seaborn\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create the figure and axis\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Use seaborn function to enhance aesthetics\n",
        "sns.histplot(red_std_yes, bins=30, color='gray', label='Corrosion', stat='density', alpha=0.7)\n",
        "sns.histplot(red_std_no, bins=30, color='black', label='No Corrosion', stat='density', alpha=0.7)\n",
        "\n",
        "# Enhance labels and titles\n",
        "#plt.title('Distribution of Homogeneity GLCM 0ยบ for Corrosion and No Corrosion', fontsize=14)\n",
        "plt.xlabel('Red Standard Deviation', fontsize=12)\n",
        "plt.ylabel('Density', fontsize=12)\n",
        "\n",
        "# Add legend with a cleaner format\n",
        "plt.legend(title='Category', fontsize=11, title_fontsize='13')\n",
        "\n",
        "# Improve plot appearance\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "# Display the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PUBJRiFkP61"
      },
      "outputs": [],
      "source": [
        "# Function to create a comparative histogram plot for multiple variables\n",
        "def create_comparative_histograms(data, variables, descriptions):\n",
        "    sns.set(style=\"whitegrid\")\n",
        "    fig, axes = plt.subplots((len(variables) + 2) // 3, 3, figsize=(18, 6 * ((len(variables) + 2) // 3)))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, var in enumerate(variables):\n",
        "        corrosion_data = data[data['etiqueta'] == 'corrosion']\n",
        "        no_corrosion_data = data[data['etiqueta'] == 'no corrosion']\n",
        "\n",
        "        var_corr = corrosion_data[var]\n",
        "        var_no_corr = no_corrosion_data[var]\n",
        "\n",
        "        sns.histplot(var_corr, bins=30, color='red', label='Corrosion', stat='density', alpha=0.6, ax=axes[i])\n",
        "        sns.histplot(var_no_corr, bins=30, color='green', label='No Corrosion', stat='density', alpha=0.6, ax=axes[i])\n",
        "\n",
        "        description = descriptions[var]\n",
        "        axes[i].set_title(f'Distribution of \"{description}\"', fontsize=14)\n",
        "        axes[i].set_xlabel(description, fontsize=12)\n",
        "        axes[i].set_ylabel('Density', fontsize=12)\n",
        "        axes[i].legend(title='Category', fontsize=11, title_fontsize='13')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Usage\n",
        "variables = ['red_mean', 'saturation_mean', 'glcm_contrast_0', 'lbp_features_1', 'hue_std', 'blue_energy']\n",
        "descriptions = {\n",
        "    'red_mean': 'Red Mean',\n",
        "    'saturation_mean': 'Saturation Mean',\n",
        "    'glcm_contrast_0': 'GLCM Contrast',\n",
        "    'lbp_features_1': 'LBP',\n",
        "    'hue_std': 'Hue Standard Deviation',\n",
        "    'blue_energy': 'Blue Energy'\n",
        "}\n",
        "\n",
        "create_comparative_histograms(data_train, variables, descriptions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0ZNy7R-kQNL"
      },
      "outputs": [],
      "source": [
        "# PCA Plot\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train = data_train.drop(columns=['etiqueta', 'n_imagen', 'etiqueta_multi'])\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=20)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "\n",
        "# Variance explained by each component\n",
        "variance_per_component = pca.explained_variance_ratio_\n",
        "\n",
        "# Plot configuration\n",
        "sns.set_context(\"talk\")  # Larger font size\n",
        "sns.set_style(\"whitegrid\")  # Style with light grid\n",
        "\n",
        "# Create the figure and axis\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot points\n",
        "x = range(1, len(variance_per_component) + 1)\n",
        "plt.scatter(x, variance_per_component, marker='o', color='blue', s=100, label='Explained Variance')\n",
        "\n",
        "# Connect points with a line\n",
        "plt.plot(x, variance_per_component, linestyle='-', color='blue', linewidth=2)\n",
        "\n",
        "# Axis labels\n",
        "plt.xlabel('Principal Components', fontsize=14, labelpad=10)\n",
        "plt.ylabel('Explained Variance', fontsize=14, labelpad=10)\n",
        "\n",
        "# Adjust limits and ticks\n",
        "plt.ylim(0, max(variance_per_component) + 0.1)  # Leave some space above\n",
        "plt.yticks(fontsize=12)\n",
        "\n",
        "# Add legend\n",
        "plt.legend(fontsize=12, loc='best')\n",
        "\n",
        "# Improved grid\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "# Save the plot with publication quality\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMPCmjC_6Cdp"
      },
      "outputs": [],
      "source": [
        "# Plot cumulative variance\n",
        "# Calculate cumulative variance\n",
        "cumulative_variance = np.cumsum(variance_per_component)\n",
        "\n",
        "# Plot configuration\n",
        "sns.set_context(\"talk\")  # Larger font size\n",
        "sns.set_style(\"whitegrid\")  # Style with light grid\n",
        "\n",
        "# Create the figure and axis\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot points\n",
        "x = range(1, len(cumulative_variance) + 1)\n",
        "plt.scatter(x, cumulative_variance, marker='o', color='blue', s=100, label='Cumulative Explained Variance')\n",
        "\n",
        "# Connect points with a line\n",
        "plt.plot(x, cumulative_variance, linestyle='-', color='blue', linewidth=2)\n",
        "\n",
        "# Axis labels\n",
        "plt.xlabel('Principal Components', fontsize=14, labelpad=10)\n",
        "plt.ylabel('Cumulative Explained Variance', fontsize=14, labelpad=10)\n",
        "\n",
        "# Adjust limits and ticks\n",
        "plt.ylim(0, 1)  # Leave some space above\n",
        "plt.yticks(fontsize=12)\n",
        "\n",
        "# Add legend\n",
        "plt.legend(fontsize=12, loc='best')\n",
        "\n",
        "# Improved grid\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "# Save the plot with publication quality\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}