{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYkvmYnl2Ydd"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import recall_score, precision_score, f1_score\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from xgboost import XGBClassifier as xgb\n",
        "from lightgbm import LGBMClassifier as lgbm\n",
        "import seaborn as sns\n",
        "from scipy.stats import mstats\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy import stats\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.tools.tools import add_constant\n",
        "import statsmodels.api as sm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from mlxtend.feature_selection import SequentialFeatureSelector as SFS"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "k_tla2ksooSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_train=pd.read_csv('path_to_train_data')\n",
        "data_test=pd.read_csv('path_to_test_data')"
      ],
      "metadata": {
        "id": "96c1CItjon8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5-FOLD Cross Validation"
      ],
      "metadata": {
        "id": "F15g5fJdRzbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create k-fold sets from the training data\n",
        "def create_kfold_sets(data_train, n_splits=5):\n",
        "    n = len(numeros_resto)\n",
        "    image_count_per_split = int(n / n_splits)\n",
        "    numbers = numeros_resto.copy()\n",
        "    random.seed(42)  # Set the random seed for reproducibility\n",
        "    random.shuffle(numbers)\n",
        "\n",
        "    # Split the numbers into k folds\n",
        "    fold1_numbers = numbers[:image_count_per_split]\n",
        "    fold2_numbers = numbers[image_count_per_split:2*image_count_per_split]\n",
        "    fold3_numbers = numbers[2*image_count_per_split:3*image_count_per_split]\n",
        "    fold4_numbers = numbers[3*image_count_per_split:4*image_count_per_split]\n",
        "    fold5_numbers = numbers[4*image_count_per_split:]\n",
        "\n",
        "    # Create data subsets for each fold\n",
        "    fold1_data = data_train[data_train['n_image'].isin(fold1_numbers)]\n",
        "    fold2_data = data_train[data_train['n_image'].isin(fold2_numbers)]\n",
        "    fold3_data = data_train[data_train['n_image'].isin(fold3_numbers)]\n",
        "    fold4_data = data_train[data_train['n_image'].isin(fold4_numbers)]\n",
        "    fold5_data = data_train[data_train['n_image'].isin(fold5_numbers)]\n",
        "\n",
        "    return fold1_data, fold2_data, fold3_data, fold4_data, fold5_data"
      ],
      "metadata": {
        "id": "tq4ry3DQR4rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X1, X2, X3, X4, X5 = create_kfold_sets(data_train=data_train)\n",
        "D = [X1, X2, X3, X4, X5]"
      ],
      "metadata": {
        "id": "GD1K39eGpJfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaI6c41OFkZe"
      },
      "source": [
        "# 1. WITHOUT FEATURE SELECTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pG-KRGf97hZD"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-4J15nmFg_n"
      },
      "outputs": [],
      "source": [
        "# Define the columns for the results DataFrame\n",
        "columns = ['Model', 'N_estimators', 'Criterion', 'Min_samples_leaf',\n",
        "           'Accuracy',\n",
        "           'Recall',\n",
        "           'Specificity',\n",
        "           'Precision',\n",
        "           'F1',\n",
        "           'Time']\n",
        "df_results = pd.DataFrame(columns=columns)\n",
        "\n",
        "# Define parameters for Grid Search\n",
        "n_estimators = [25, 50, 75, 100]\n",
        "criterion = ['gini', 'entropy']\n",
        "min_samples_leaf = [1, 2, 4, 8, 16, 32]\n",
        "\n",
        "for i in n_estimators:\n",
        "    for k in criterion:\n",
        "        for d in min_samples_leaf:\n",
        "            recalls = []\n",
        "            precisions = []\n",
        "            f1s = []\n",
        "            accuracies = []\n",
        "            specificities = []\n",
        "            times = []\n",
        "            for j in range(5):\n",
        "                # Prepare test and train sets for this fold\n",
        "                d_test = pd.concat([D[j]])\n",
        "                y_test = d_test['label_binary']\n",
        "                X_test = d_test.drop(columns=['label_binary', 'n_image', 'label_multi'])\n",
        "                d_train = pd.concat([D[k] for k in range(5) if k != j], ignore_index=True)\n",
        "                y_train = d_train['label_binary']\n",
        "                X_train = d_train.drop(columns=['label_binary', 'n_image', 'label_multi'])\n",
        "\n",
        "                # Initialize and fit the model\n",
        "                model = RandomForestClassifier(\n",
        "                    n_estimators=i,\n",
        "                    criterion=k,\n",
        "                    min_samples_leaf=d,\n",
        "                    bootstrap=True,\n",
        "                    random_state=42)\n",
        "                model.fit(X_train, y_train)\n",
        "\n",
        "                # Measure execution time\n",
        "                t0 = time.time()\n",
        "                y_pred = model.predict(X_test)\n",
        "                print(f'Model with {i} estimators:')\n",
        "                labels = ('corrosion', 'no corrosion')\n",
        "                cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
        "                print(f'Confusion matrix for {i} estimators: \\n {cm}')\n",
        "\n",
        "                # Calculate metrics\n",
        "                accuracy = accuracy_score(y_test, y_pred)\n",
        "                accuracies.append(accuracy)\n",
        "                recall = recall_score(y_test, y_pred, average='binary', pos_label='corrosion')\n",
        "                recall = round(recall, 3)\n",
        "                print(f'Recall for {i} estimators: {recall}')\n",
        "                recalls.append(recall)\n",
        "                specificity = recall_score(y_test, y_pred, average='binary', pos_label='no corrosion')\n",
        "                specificity = round(specificity, 3)\n",
        "                specificities.append(specificity)\n",
        "                print(f'Specificity: {specificity}')\n",
        "                precision = precision_score(y_test, y_pred, average='binary', pos_label='corrosion')\n",
        "                precision = round(precision, 3)\n",
        "                precisions.append(precision)\n",
        "                f1 = f1_score(y_test, y_pred, average='binary', pos_label='corrosion')\n",
        "                f1 = round(f1, 3)\n",
        "                f1s.append(f1)\n",
        "                t1 = time.time()\n",
        "                time_taken = t1 - t0\n",
        "                time_taken = round(time_taken, 3)\n",
        "                times.append(time_taken)\n",
        "                print(f'Execution time: {time_taken} seconds')\n",
        "                print('\\n')\n",
        "\n",
        "            # Calculate mean metrics across folds\n",
        "            recall_mean = np.mean(recalls)\n",
        "            specificity_mean = np.mean(specificities)\n",
        "            precision_mean = np.mean(precisions)\n",
        "            f1_mean = np.mean(f1s)\n",
        "            accuracy_mean = np.mean(accuracies)\n",
        "            time_mean = np.mean(times)\n",
        "\n",
        "            # Append results to DataFrame\n",
        "            result_i = {'Model': 'Random Forest', 'Accuracy': accuracy_mean, 'N_estimators': i, 'Criterion': k, 'Min_samples_leaf': d,\n",
        "                        'Recall': recall_mean, 'Specificity': specificity_mean, 'Precision': precision_mean, 'F1': f1_mean,\n",
        "                        'Time': time_mean}\n",
        "            df_results = pd.concat([df_results, pd.DataFrame([result_i])], ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvdHCWhVkwba"
      },
      "outputs": [],
      "source": [
        "df_resultados.sort_values(by='Recall', ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oYrrxnwfohK"
      },
      "source": [
        "Chosen model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVblk2QLfztO"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "min_samples = 1\n",
        "n_estimators = 50\n",
        "criterion = 'entropy'\n",
        "\n",
        "# Assuming data_train and data_test are already defined\n",
        "X_train = data_train.drop(columns=['label_binary', 'n_image', 'label_multi'])\n",
        "y_train = data_train['label_binary']\n",
        "X_test = data_test.drop(columns=['label_binary', 'n_image', 'label_multi'])\n",
        "y_test = data_test['label_binary']\n",
        "\n",
        "# Initialize and train the model\n",
        "model = RandomForestClassifier(\n",
        "    n_estimators=n_estimators,\n",
        "    criterion=criterion,\n",
        "    min_samples_leaf=min_samples,\n",
        "    bootstrap=True,\n",
        "    random_state=42\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Measure execution time\n",
        "t0 = time.time()\n",
        "y_pred = model.predict(X_test)\n",
        "t1 = time.time()\n",
        "\n",
        "# Metrics calculation\n",
        "labels = ('corrosion', 'no corrosion')\n",
        "cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
        "print(f'Model with {n_estimators} estimators:')\n",
        "print(f'Confusion matrix for {n_estimators} estimators: \\n {cm}')\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy for {n_estimators} estimators: {accuracy}')\n",
        "recall = recall_score(y_test, y_pred, average='binary', pos_label='corrosion')\n",
        "recall = round(recall, 3)\n",
        "print(f'Recall for {n_estimators} estimators: {recall}')\n",
        "specificity = recall_score(y_test, y_pred, average='binary', pos_label='no corrosion')\n",
        "specificity = round(specificity, 3)\n",
        "print(f'Specificity: {specificity}')\n",
        "precision = precision_score(y_test, y_pred, average='binary', pos_label='corrosion')\n",
        "precision = round(precision, 3)\n",
        "print(f'Precision for {n_estimators} estimators: {precision}')\n",
        "f1 = f1_score(y_test, y_pred, average='binary', pos_label='corrosion')\n",
        "f1 = round(f1, 3)\n",
        "print(f'F1 for {n_estimators} estimators: {f1}')\n",
        "time_taken = t1 - t0\n",
        "time_taken = round(time_taken, 3)\n",
        "print(f'Execution time: {time_taken} seconds')\n",
        "\n",
        "# Create a DataFrame to store results\n",
        "columns = ['Model', 'N_estimators', 'Criterion', 'Min_samples_leaf',\n",
        "           'Accuracy',\n",
        "           'Recall',\n",
        "           'Specificity',\n",
        "           'Precision',\n",
        "           'F1',\n",
        "           'Time']\n",
        "df_results = pd.DataFrame(columns=columns)\n",
        "\n",
        "# Append results to DataFrame\n",
        "result_rf = {'Model': 'Random Forest', 'Accuracy': accuracy, 'N_estimators': n_estimators, 'Criterion': criterion, 'Min_samples_leaf': min_samples,\n",
        "             'Recall': recall, 'Specificity': specificity, 'Precision': precision, 'F1': f1,\n",
        "             'Time': time_taken}\n",
        "df_results = pd.concat([df_results, pd.DataFrame([result_rf])], ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ra1-b1YRhDek"
      },
      "outputs": [],
      "source": [
        "# Get feature importances from the trained model\n",
        "importancias = model.feature_importances_\n",
        "\n",
        "# Plot feature importances as a bar chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(X_train.columns, importancias, color='skyblue')\n",
        "plt.xticks(rotation=90, fontsize=10)\n",
        "plt.xlabel('Features', fontsize=12)\n",
        "plt.ylabel('Importance', fontsize=12)\n",
        "plt.title('Feature Importances from Random Forest', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlQIVqneWKIU"
      },
      "outputs": [],
      "source": [
        "# Create a DataFrame from feature importances\n",
        "importancias_df = pd.DataFrame(importancias, index=X_train.columns, columns=['Importance'])\n",
        "\n",
        "# Sort the DataFrame by importance in descending order\n",
        "sorted_importances_df = importancias_df.sort_values(by='Importance', ascending=False)\n",
        "print(sorted_importances_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjIRNQ1o7l-0"
      },
      "source": [
        "## XGBOOST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azY4E8Dx73yQ"
      },
      "outputs": [],
      "source": [
        "# Define the columns for the results DataFrame\n",
        "columns = ['Model', 'N_estimators', 'Learning_rate', 'Min_child_weight',\n",
        "           'Accuracy',\n",
        "           'Recall',\n",
        "           'Specificity',\n",
        "           'Precision',\n",
        "           'F1',\n",
        "           'Time']\n",
        "df_results = pd.DataFrame(columns=columns)\n",
        "\n",
        "# Define parameters for Grid Search\n",
        "n_estimators = [25, 50, 75, 100, 125, 150]\n",
        "metric = ['logloss']\n",
        "learning_rates = [0.01, 0.05, 0.1, 0.5]\n",
        "min_child_weight = [1, 2, 4, 8, 16, 32]\n",
        "\n",
        "for i in n_estimators:\n",
        "    for d in learning_rates:\n",
        "        for k in min_child_weight:\n",
        "            for m in metric:\n",
        "                recalls = []\n",
        "                specificities = []\n",
        "                precisions = []\n",
        "                f1s = []\n",
        "                accuracies = []\n",
        "                times = []\n",
        "                for j in range(5):\n",
        "                    d_test = pd.concat([D[j]])\n",
        "                    y_test = d_test['label_binary']\n",
        "                    X_test = d_test.drop(columns=['label_binary', 'n_image', 'label_multi'])\n",
        "                    d_train = pd.concat([D[k] for k in range(5) if k != j], ignore_index=True)\n",
        "                    y_train = d_train['label_binary']\n",
        "                    X_train = d_train.drop(columns=['label_binary', 'n_image', 'label_multi'])\n",
        "                    mapping = {'no corrosion': 0, 'corrosion': 1}\n",
        "                    y_train = y_train.map(mapping)\n",
        "                    y_test = y_test.map(mapping)\n",
        "\n",
        "                    # Initialize and train the XGBoost model\n",
        "                    model = xgb(\n",
        "                        objective='binary:logistic',  # Binary classification\n",
        "                        n_estimators=i,             # Number of trees (boosting rounds)\n",
        "                        seed=42,                       # For reproducibility\n",
        "                        learning_rate=d,\n",
        "                        min_child_weight=k,\n",
        "                        eval_metric=m\n",
        "                    )\n",
        "                    model.fit(X_train, y_train)\n",
        "\n",
        "                    # Measure execution time\n",
        "                    t0 = time.time()\n",
        "                    y_pred = model.predict(X_test)\n",
        "                    print(f'Model with {i} estimators, {d} learning rate, and {k} min child weight:')\n",
        "                    labels = (1, 0)\n",
        "                    cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
        "                    print(f'Confusion matrix for {i} estimators: \\n {cm}')\n",
        "\n",
        "                    # Calculate metrics\n",
        "                    accuracy = accuracy_score(y_test, y_pred)\n",
        "                    accuracies.append(accuracy)\n",
        "                    recall = recall_score(y_test, y_pred, average='binary')\n",
        "                    recall = round(recall, 3)\n",
        "                    print(f'Recall for {i} estimators: {recall}')\n",
        "                    recalls.append(recall)\n",
        "                    specificity = recall_score(y_test, y_pred, average='binary', pos_label=0)\n",
        "                    specificity = round(specificity, 3)\n",
        "                    specificities.append(specificity)\n",
        "                    print(f'Specificity: {specificity}')\n",
        "                    precision = precision_score(y_test, y_pred, average='binary')\n",
        "                    precision = round(precision, 3)\n",
        "                    precisions.append(precision)\n",
        "                    f1 = f1_score(y_test, y_pred, average='binary')\n",
        "                    f1 = round(f1, 3)\n",
        "                    f1s.append(f1)\n",
        "                    t1 = time.time()\n",
        "                    time_taken = t1 - t0\n",
        "                    time_taken = round(time_taken, 3)\n",
        "                    times.append(time_taken)\n",
        "                    print(f'Execution time: {time_taken} seconds')\n",
        "                    print('\\n')\n",
        "\n",
        "                # Calculate mean metrics across folds\n",
        "                recall_mean = np.mean(recalls)\n",
        "                specificity_mean = np.mean(specificities)\n",
        "                precision_mean = np.mean(precisions)\n",
        "                f1_mean = np.mean(f1s)\n",
        "                accuracy_mean = np.mean(accuracies)\n",
        "                time_mean = np.mean(times)\n",
        "\n",
        "                # Append results to DataFrame\n",
        "                result_i = {'Model': 'XGBoost', 'Accuracy': accuracy_mean, 'N_estimators': i, 'Learning_rate': d, 'Min_child_weight': k,\n",
        "                            'Recall': recall_mean, 'Specificity': specificity_mean,\n",
        "                            'Precision': precision_mean, 'F1': f1_mean,\n",
        "                            'Time': time_mean}\n",
        "                df_results = pd.concat([df_results, pd.DataFrame([result_i])], ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gil3vlBz73yR"
      },
      "outputs": [],
      "source": [
        "df_results.sort_values(by='Recall', ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gk-8m_bagbo7"
      },
      "source": [
        "Chosen model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SAh-SSxgnRY"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "n_estimators = 75\n",
        "learning_rate = 0.01\n",
        "min_child_weight = 32\n",
        "\n",
        "# Initialize and train the XGBoost model\n",
        "model = xgb(\n",
        "    objective='binary:logistic',  # Binary classification\n",
        "    n_estimators=n_estimators,     # Number of trees (boosting rounds)\n",
        "    seed=42,                       # For reproducibility\n",
        "    learning_rate=learning_rate,\n",
        "    min_child_weight=min_child_weight,\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "\n",
        "# Prepare data\n",
        "X_train = data_train.drop(columns=['label_binary', 'n_image', 'label_multi'])\n",
        "y_train = data_train['label_binary']\n",
        "X_test = data_test.drop(columns=['label_binary', 'n_image', 'label_multi'])\n",
        "y_test = data_test['label_binary']\n",
        "\n",
        "# Map labels to numeric values\n",
        "label_mapping = {'no corrosion': 0, 'corrosion': 1}\n",
        "y_train = y_train.map(label_mapping)\n",
        "y_test = y_test.map(label_mapping)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Measure execution time\n",
        "t0 = time.time()\n",
        "y_pred = model.predict(X_test)\n",
        "t1 = time.time()\n",
        "\n",
        "# Print model details and metrics\n",
        "print(f'Model with {n_estimators} estimators, {learning_rate} learning rate, and {min_child_weight} min child weight:')\n",
        "labels = (1, 0)\n",
        "cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
        "print(f'Confusion matrix for {n_estimators} estimators: \\n {cm}')\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy for {n_estimators} estimators: {accuracy}')\n",
        "recall = recall_score(y_test, y_pred, average='binary')\n",
        "recall = round(recall, 3)\n",
        "print(f'Recall for {n_estimators} estimators: {recall}')\n",
        "specificity = recall_score(y_test, y_pred, average='binary', pos_label=0)\n",
        "specificity = round(specificity, 3)\n",
        "print(f'Specificity: {specificity}')\n",
        "precision = precision_score(y_test, y_pred, average='binary')\n",
        "precision = round(precision, 3)\n",
        "print(f'Precision for {n_estimators} estimators: {precision}')\n",
        "f1 = f1_score(y_test, y_pred, average='binary')\n",
        "f1 = round(f1, 3)\n",
        "print(f'F1 for {n_estimators} estimators: {f1}')\n",
        "time_taken = t1 - t0\n",
        "time_taken = round(time_taken, 3)\n",
        "print(f'Execution time: {time_taken} seconds')\n",
        "\n",
        "# Create a DataFrame to store results\n",
        "columns = ['Model', 'N_estimators', 'Learning_rate', 'Min_child_weight', 'Accuracy',\n",
        "           'Recall',\n",
        "           'Specificity',\n",
        "           'Precision',\n",
        "           'F1',\n",
        "           'Time']\n",
        "df_results = pd.DataFrame(columns=columns)\n",
        "\n",
        "# Append results to DataFrame\n",
        "result_xgb = {'Model': 'XGBoost', 'Accuracy': accuracy, 'N_estimators': n_estimators, 'Learning_rate': learning_rate, 'Min_child_weight': min_child_weight,\n",
        "              'Recall': recall, 'Specificity': specificity, 'Precision': precision, 'F1': f1,\n",
        "              'Time': time_taken}\n",
        "df_results = pd.concat([df_results, pd.DataFrame([result_xgb])], ignore_index=True)\n",
        "print(df_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NIhJRmHB_Gu"
      },
      "source": [
        "## LIGHTGBM GBDT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KoqoQO_CCT2"
      },
      "outputs": [],
      "source": [
        "# Define the columns for the results DataFrame\n",
        "columns = ['Model', 'N_estimators', 'Learning_rate', 'N_bins', 'Num_leaves', 'Accuracy',\n",
        "           'Metric',\n",
        "           'Recall',\n",
        "           'Specificity',\n",
        "           'Precision',\n",
        "           'F1',\n",
        "           'Time']\n",
        "df_results = pd.DataFrame(columns=columns)\n",
        "\n",
        "# Define parameters for Grid Search\n",
        "n_estimators = [25, 50, 75, 100, 125, 150]\n",
        "metric = ['logloss']\n",
        "learning_rates = [0.025, 0.05, 0.1, 0.2, 0.4]\n",
        "n_bins = [255]\n",
        "num_leaves = [10, 30, 50]\n",
        "\n",
        "for i in n_estimators:\n",
        "    for d in learning_rates:\n",
        "        for k in n_bins:\n",
        "            for l in num_leaves:\n",
        "                for m in metric:\n",
        "                    recalls = []\n",
        "                    specificities = []\n",
        "                    precisions = []\n",
        "                    f1s = []\n",
        "                    accuracies = []\n",
        "                    times = []\n",
        "                    for j in range(5):\n",
        "                        # Prepare test and train sets for this fold\n",
        "                        d_test = pd.concat([D[j]])\n",
        "                        y_test = d_test['label_binary']\n",
        "                        X_test = d_test.drop(columns=['label_binary', 'n_image', 'label_multi'])\n",
        "                        d_train = pd.concat([D[k] for k in range(5) if k != j], ignore_index=True)\n",
        "                        y_train = d_train['label_binary']\n",
        "                        X_train = d_train.drop(columns=['label_binary', 'n_image', 'label_multi'])\n",
        "                        label_mapping = {'no corrosion': 0, 'corrosion': 1}\n",
        "                        y_train = y_train.map(label_mapping)\n",
        "                        y_test = y_test.map(label_mapping)\n",
        "\n",
        "                        # Initialize and train the LightGBM model\n",
        "                        model = lgbm(\n",
        "                            objective='binary',  # Binary classification\n",
        "                            boosting_type='gbdt',  # Boosting type\n",
        "                            n_estimators=i,             # Number of trees (boosting rounds)\n",
        "                            seed=42,                       # For reproducibility\n",
        "                            learning_rate=d,\n",
        "                            n_bins=k,\n",
        "                            num_leaves=l,\n",
        "                            eval_metric=m,\n",
        "                            verbose=-1\n",
        "                        )\n",
        "                        model.fit(X_train, y_train)\n",
        "\n",
        "                        # Measure execution time\n",
        "                        t0 = time.time()\n",
        "                        y_pred = model.predict(X_test)\n",
        "                        print(f'Model with {i} estimators, {d} learning rate, and {l} leaves:')\n",
        "                        labels = (1, 0)\n",
        "                        cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
        "                        print(f'Confusion matrix for {i} estimators: \\n {cm}')\n",
        "\n",
        "                        # Calculate metrics\n",
        "                        accuracy = accuracy_score(y_test, y_pred)\n",
        "                        accuracies.append(accuracy)\n",
        "                        recall = recall_score(y_test, y_pred, average='binary')\n",
        "                        recall = round(recall, 3)\n",
        "                        print(f'Recall for {i} estimators: {recall}')\n",
        "                        recalls.append(recall)\n",
        "                        specificity = recall_score(y_test, y_pred, average='binary', pos_label=0)\n",
        "                        specificity = round(specificity, 3)\n",
        "                        specificities.append(specificity)\n",
        "                        print(f'Specificity: {specificity}')\n",
        "                        precision = precision_score(y_test, y_pred, average='binary')\n",
        "                        precision = round(precision, 3)\n",
        "                        precisions.append(precision)\n",
        "                        f1 = f1_score(y_test, y_pred, average='binary')\n",
        "                        f1 = round(f1, 3)\n",
        "                        f1s.append(f1)\n",
        "                        t1 = time.time()\n",
        "                        time_taken = t1 - t0\n",
        "                        time_taken = round(time_taken, 3)\n",
        "                        times.append(time_taken)\n",
        "                        print(f'Execution time: {time_taken} seconds')\n",
        "                        print('\\n')\n",
        "\n",
        "                    # Calculate mean metrics across folds\n",
        "                    recall_mean = np.mean(recalls)\n",
        "                    specificity_mean = np.mean(specificities)\n",
        "                    precision_mean = np.mean(precisions)\n",
        "                    f1_mean = np.mean(f1s)\n",
        "                    accuracy_mean = np.mean(accuracies)\n",
        "                    time_mean = np.mean(times)\n",
        "\n",
        "                    # Append results to DataFrame\n",
        "                    result_i = {'Model': 'LGBM', 'Accuracy': accuracy_mean, 'N_estimators': i, 'Learning_rate': d, 'N_bins': k, 'Num_leaves': l,\n",
        "                                'Metric': m,\n",
        "                                'Recall': recall_mean, 'Specificity': specificity_mean,\n",
        "                                'Precision': precision_mean, 'F1': f1_mean,\n",
        "                                'Time': time_mean}\n",
        "                    df_results = pd.concat([df_results, pd.DataFrame([result_i])], ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuWQOnNYkwaq"
      },
      "outputs": [],
      "source": [
        "df_resultados.sort_values(by='Recall', ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chosen model"
      ],
      "metadata": {
        "id": "_NyBqO5GUe8g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqUXR166DvEB"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "n_estimators = 25\n",
        "learning_rate = 0.025\n",
        "n_bins = 255\n",
        "num_leaves = 10\n",
        "\n",
        "# Prepare data\n",
        "X_train = data_train.drop(columns=['label_binary', 'n_image', 'label_multi'])\n",
        "y_train = data_train['label_binary']\n",
        "label_mapping = {'no corrosion': 0, 'corrosion': 1}\n",
        "y_train = y_train.map(label_mapping)\n",
        "X_test = data_test.drop(columns=['label_binary', 'n_image', 'label_multi'])\n",
        "y_test = data_test['label_binary']\n",
        "y_test = y_test.map(label_mapping)\n",
        "\n",
        "# Initialize and train the LightGBM model\n",
        "model = lgbm(\n",
        "    objective='binary',  # Binary classification\n",
        "    boosting_type='gbdt',  # Boosting type\n",
        "    n_estimators=n_estimators,             # Number of trees (boosting rounds)\n",
        "    seed=42,                       # For reproducibility\n",
        "    learning_rate=learning_rate,\n",
        "    eval_metric='logloss',\n",
        "    verbose=-1,\n",
        "    n_bins=n_bins,\n",
        "    num_leaves=num_leaves\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Measure execution time\n",
        "t0 = time.time()\n",
        "y_pred = model.predict(X_test)\n",
        "t1 = time.time()\n",
        "time_taken = t1 - t0\n",
        "time_taken = round(time_taken, 3)\n",
        "\n",
        "# Print model details and metrics\n",
        "print(f'Model with {n_estimators} estimators:')\n",
        "labels = (1, 0)\n",
        "cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
        "print(f'Confusion matrix for estimators: \\n {cm}')\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred, average='binary')\n",
        "recall = round(recall, 3)\n",
        "print(f'Recall for estimators: {recall}')\n",
        "print(f'Execution time: {time_taken} seconds')\n",
        "precision = precision_score(y_test, y_pred, average='binary')\n",
        "precision = round(precision, 3)\n",
        "specificity = recall_score(y_test, y_pred, average='binary', pos_label=0)\n",
        "specificity = round(specificity, 3)\n",
        "f1 = f1_score(y_test, y_pred, average='binary')\n",
        "f1 = round(f1, 3)\n",
        "print(f'Specificity: {specificity}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'F1: {f1}')\n",
        "\n",
        "# Create a DataFrame to store results\n",
        "columns = ['Model', 'N_estimators', 'Accuracy', 'Learning_rate', 'N_bins', 'Num_leaves',\n",
        "           'Metric',\n",
        "           'Recall',\n",
        "           'Specificity',\n",
        "           'Precision',\n",
        "           'F1',\n",
        "           'Time']\n",
        "df_results = pd.DataFrame(columns=columns)\n",
        "\n",
        "# Append results to DataFrame\n",
        "result_i = {'Model': 'LightGBM', 'N_estimators': n_estimators, 'Learning_rate': learning_rate, 'N_bins': n_bins, 'Num_leaves': num_leaves,\n",
        "            'Metric': 'logloss', 'Accuracy': accuracy,\n",
        "            'Recall': recall, 'Specificity': specificity,\n",
        "            'Precision': precision, 'F1': f1,\n",
        "            'Time': time_taken}\n",
        "df_results = pd.concat([df_results, pd.DataFrame([result_i])], ignore_index=True)\n",
        "\n",
        "print(df_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDz2mEPvDwN_"
      },
      "source": [
        "## LIGHTGBM GOSS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UkTi-GgDvTa"
      },
      "outputs": [],
      "source": [
        "# Define the columns for the results DataFrame\n",
        "columns = ['Model', 'Top_rate', 'Other_rate', 'Learning_rate', 'Num_leaves',\n",
        "           'Accuracy',\n",
        "           'Recall',\n",
        "           'Specificity',\n",
        "           'Precision',\n",
        "           'F1',\n",
        "           'Time']\n",
        "df_results = pd.DataFrame(columns=columns)\n",
        "\n",
        "# Define parameters for Grid Search\n",
        "top_rate = [0.2, 0.4, 0.6]\n",
        "other_rate = [0.05, 0.1, 0.3]  # Corrected to be a list\n",
        "learning_rates = [0.025, 0.05, 0.1, 0.2]\n",
        "num_leaves = [10, 30, 50]\n",
        "\n",
        "for i in top_rate:\n",
        "    for d in learning_rates:\n",
        "        for k in other_rate:\n",
        "            for l in num_leaves:\n",
        "                recalls = []\n",
        "                specificities = []\n",
        "                precisions = []\n",
        "                f1s = []\n",
        "                accuracies = []\n",
        "                times = []\n",
        "                for j in range(5):\n",
        "                    # Prepare test and train sets for this fold\n",
        "                    d_test = pd.concat([D[j]])\n",
        "                    y_test = d_test['label_binary']\n",
        "                    X_test = d_test.drop(columns=['label_binary', 'n_image', 'label_multi'])\n",
        "                    d_train = pd.concat([D[k] for k in range(5) if k != j], ignore_index=True)\n",
        "                    y_train = d_train['label_binary']\n",
        "                    X_train = d_train.drop(columns=['label_binary', 'n_image', 'label_multi'])\n",
        "                    label_mapping = {'no corrosion': 0, 'corrosion': 1}\n",
        "                    y_train = y_train.map(label_mapping)\n",
        "                    y_test = y_test.map(label_mapping)\n",
        "\n",
        "                    # Initialize and train the LightGBM model\n",
        "                    model = lgbm(\n",
        "                        objective='binary',  # Binary classification\n",
        "                        boosting_type='goss',  # Boosting type\n",
        "                        top_rate=i,             # Top rate\n",
        "                        seed=42,                       # For reproducibility\n",
        "                        learning_rate=d,\n",
        "                        other_rate=k,\n",
        "                        num_leaves=l,\n",
        "                        verbose=-1\n",
        "                    )\n",
        "                    model.fit(X_train, y_train)\n",
        "\n",
        "                    # Measure execution time\n",
        "                    t0 = time.time()\n",
        "                    y_pred = model.predict(X_test)\n",
        "                    print(f'Model with {i} top rate, {d} learning rate, and {l} leaves:')\n",
        "                    labels = (1, 0)\n",
        "                    cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
        "                    print(f'Confusion matrix for {i} top rate: \\n {cm}')\n",
        "\n",
        "                    # Calculate metrics\n",
        "                    accuracy = accuracy_score(y_test, y_pred)\n",
        "                    accuracies.append(accuracy)\n",
        "                    recall = recall_score(y_test, y_pred, average='binary')\n",
        "                    recall = round(recall, 3)\n",
        "                    print(f'Recall for {i} top rate: {recall}')\n",
        "                    recalls.append(recall)\n",
        "                    specificity = recall_score(y_test, y_pred, average='binary', pos_label=0)\n",
        "                    specificity = round(specificity, 3)\n",
        "                    specificities.append(specificity)\n",
        "                    print(f'Specificity: {specificity}')\n",
        "                    precision = precision_score(y_test, y_pred, average='binary')\n",
        "                    precision = round(precision, 3)\n",
        "                    precisions.append(precision)\n",
        "                    f1 = f1_score(y_test, y_pred, average='binary')\n",
        "                    f1 = round(f1, 3)\n",
        "                    f1s.append(f1)\n",
        "                    t1 = time.time()\n",
        "                    time_taken = t1 - t0\n",
        "                    time_taken = round(time_taken, 3)\n",
        "                    times.append(time_taken)\n",
        "                    print(f'Execution time: {time_taken} seconds')\n",
        "                    print('\\n')\n",
        "\n",
        "                # Calculate mean metrics across folds\n",
        "                recall_mean = np.mean(recalls)\n",
        "                specificity_mean = np.mean(specificities)\n",
        "                precision_mean = np.mean(precisions)\n",
        "                f1_mean = np.mean(f1s)\n",
        "                accuracy_mean = np.mean(accuracies)\n",
        "                time_mean = np.mean(times)\n",
        "\n",
        "                # Append results to DataFrame\n",
        "                result_i = {'Model': 'LightGBM', 'Accuracy': accuracy_mean, 'Top_rate': i, 'Learning_rate': d, 'Other_rate': k, 'Num_leaves': l,\n",
        "                            'Recall': recall_mean, 'Specificity': specificity_mean,\n",
        "                            'Precision': precision_mean, 'F1': f1_mean,\n",
        "                            'Time': time_mean}\n",
        "                df_results = pd.concat([df_results, pd.DataFrame([result_i])], ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmI7dxKgDvTb"
      },
      "outputs": [],
      "source": [
        "df_resultados.sort_values(by='Recall', ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chosen model"
      ],
      "metadata": {
        "id": "xr4lPKeiVEjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model parameters\n",
        "top_rate = 0.2\n",
        "learning_rate = 0.05\n",
        "other_rate = 0.05\n",
        "num_leaves = 10\n",
        "\n",
        "# Prepare training and testing data\n",
        "X_train = data_train.drop(columns=['label_binary', 'label_multi', 'n_image'])\n",
        "y_train = data_train['label_binary']\n",
        "label_mapping = {'no corrosion': 0, 'corrosion': 1}\n",
        "y_train = y_train.map(label_mapping)\n",
        "\n",
        "X_test = data_test.drop(columns=['label_binary', 'label_multi', 'n_image'])\n",
        "y_test = data_test['label_binary']\n",
        "y_test = y_test.map(label_mapping)\n",
        "\n",
        "# Initialize and train the LightGBM model\n",
        "model = lgbm(\n",
        "    objective='binary',  # Binary classification\n",
        "    boosting_type='goss',  # Boosting type\n",
        "    top_rate=top_rate,             # Top rate\n",
        "    seed=42,                       # For reproducibility\n",
        "    learning_rate=learning_rate,\n",
        "    other_rate=other_rate,\n",
        "    num_leaves=num_leaves,\n",
        "    verbose=-1\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Measure execution time\n",
        "t0 = time.time()\n",
        "y_pred = model.predict(X_test)\n",
        "t1 = time.time()\n",
        "\n",
        "# Print model details and metrics\n",
        "print(f'Model with {num_leaves} leaves, {learning_rate} learning rate, {top_rate} top rate, and {other_rate} other rate:')\n",
        "labels = (1, 0)\n",
        "cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
        "print(f'Confusion matrix: \\n {cm}')\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy}')\n",
        "recall = recall_score(y_test, y_pred, average='binary')\n",
        "recall = round(recall, 3)\n",
        "print(f'Recall: {recall}')\n",
        "specificity = recall_score(y_test, y_pred, average='binary', pos_label=0)\n",
        "specificity = round(specificity, 3)\n",
        "print(f'Specificity: {specificity}')\n",
        "precision = precision_score(y_test, y_pred, average='binary')\n",
        "precision = round(precision, 3)\n",
        "print(f'Precision: {precision}')\n",
        "f1 = f1_score(y_test, y_pred, average='binary')\n",
        "f1 = round(f1, 3)\n",
        "print(f'F1: {f1}')\n",
        "time_taken = t1 - t0\n",
        "time_taken = round(time_taken, 3)\n",
        "print(f'Execution time: {time_taken} seconds')\n",
        "print('\\n')\n",
        "\n",
        "# Create a DataFrame to store results\n",
        "columns = ['Model', 'Learning_rate', 'Num_leaves', 'Top_rate', 'Other_rate',\n",
        "           'Accuracy',\n",
        "           'Recall',\n",
        "           'Specificity',\n",
        "           'Precision',\n",
        "           'F1',\n",
        "           'Time']\n",
        "df_results = pd.DataFrame(columns=columns)\n",
        "\n",
        "# Append results to DataFrame\n",
        "result_lgbm = {'Model': 'LightGBM', 'Accuracy': accuracy, 'Top_rate': top_rate, 'Other_rate': other_rate, 'Learning_rate': learning_rate, 'Num_leaves': num_leaves,\n",
        "               'Recall': recall, 'Specificity': specificity, 'Precision': precision, 'F1': f1,\n",
        "               'Time': time_taken}\n",
        "df_results = pd.concat([df_results, pd.DataFrame([result_lgbm])], ignore_index=True)\n",
        "print(df_results)\n"
      ],
      "metadata": {
        "id": "_DQdQzwnV6wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfzU4XpWTsKl"
      },
      "source": [
        "## KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egsiaHAATx01"
      },
      "outputs": [],
      "source": [
        "# Define the columns for the results DataFrame\n",
        "columns = ['Model', 'N_neighbors',  'Distance', 'Accuracy',\n",
        "           'Recall',\n",
        "           'Specificity',\n",
        "           'Precision',\n",
        "           'F1',\n",
        "           'Time']\n",
        "df_results = pd.DataFrame(columns=columns)\n",
        "\n",
        "# List of neighbors to test\n",
        "N_neighbors = [1, 3, 5, 11, 21, 41, 61]\n",
        "\n",
        "# List of distance metrics to use\n",
        "distance_metrics = ['euclidean', 'manhattan']\n",
        "\n",
        "# Iterate over each number of neighbors and distance metric\n",
        "for i in N_neighbors:\n",
        "    for d in distance_metrics:\n",
        "        # Lists to store results across multiple iterations\n",
        "        recalls = []\n",
        "        specificities = []\n",
        "        precisions = []\n",
        "        f1s = []\n",
        "        accuracies = []\n",
        "        times = []\n",
        "\n",
        "        # Perform cross-validation by iterating over each fold\n",
        "        for j in range(5):\n",
        "\n",
        "            # Select the test dataset for this iteration\n",
        "            d_test = pd.concat([D[j]])\n",
        "            y_test = d_test['label_binary']\n",
        "            X_test = d_test.drop(columns=['label_binary', 'n_image', 'label_multi'])\n",
        "\n",
        "            # Select the training datasets for this iteration\n",
        "            d_train = pd.concat([D[k] for k in range(5) if k != j], ignore_index=True)\n",
        "            y_train = d_train['label_binary']\n",
        "            X_train = d_train.drop(columns=['label_binary', 'n_image', 'label_multi'])\n",
        "\n",
        "            # Scale the data using StandardScaler\n",
        "            scaler = StandardScaler()\n",
        "            X_train = scaler.fit_transform(X_train)\n",
        "            X_test = scaler.transform(X_test)\n",
        "\n",
        "            # Initialize and fit the KNeighborsClassifier model\n",
        "            model = KNeighborsClassifier(n_neighbors=i, metric=d)\n",
        "            model.fit(X_train, y_train)\n",
        "\n",
        "            # Measure the time taken to make predictions\n",
        "            t0 = time.time()\n",
        "            y_pred = model.predict(X_test)\n",
        "            t1 = time.time()\n",
        "\n",
        "            # Print model details\n",
        "            print(f'Model with {i} neighbors using {d} distance:')\n",
        "            labels = ('corrosion', 'no corrosion')\n",
        "            cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
        "            print(f'Confusion matrix for {i} neighbors:\\n {cm}')\n",
        "\n",
        "            # Calculate and store accuracy\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "            accuracies.append(accuracy)\n",
        "\n",
        "            # Calculate and store recall\n",
        "            recall = recall_score(y_test, y_pred, average='binary', pos_label='corrosion')\n",
        "            recall = round(recall, 3)\n",
        "            print(f'Recall for {i} neighbors: {recall}')\n",
        "            recalls.append(recall)\n",
        "\n",
        "            # Calculate and store specificity\n",
        "            specificity = recall_score(y_test, y_pred, average='binary', pos_label='no corrosion')\n",
        "            specificity = round(specificity, 3)\n",
        "            print(f'Specificity: {specificity}')\n",
        "            specificities.append(specificity)\n",
        "\n",
        "            # Calculate and store precision\n",
        "            precision = precision_score(y_test, y_pred, average='binary', pos_label='corrosion')\n",
        "            precision = round(precision, 3)\n",
        "            precisions.append(precision)\n",
        "\n",
        "            # Calculate and store F1 score\n",
        "            f1 = f1_score(y_test, y_pred, average='binary', pos_label='corrosion')\n",
        "            f1 = round(f1, 3)\n",
        "            f1s.append(f1)\n",
        "\n",
        "            # Calculate and store time taken\n",
        "            time_taken = t1 - t0\n",
        "            time_taken = round(time_taken, 3)\n",
        "            times.append(time_taken)\n",
        "            print(f'Execution time: {time_taken} seconds')\n",
        "            print('\\n')\n",
        "\n",
        "        # Calculate mean metrics across all iterations\n",
        "        recall_mean = np.mean(recalls)\n",
        "        specificity_mean = np.mean(specificities)\n",
        "        precision_mean = np.mean(precisions)\n",
        "        f1_mean = np.mean(f1s)\n",
        "        accuracy_mean = np.mean(accuracies)\n",
        "        time_mean = np.mean(times)\n",
        "\n",
        "        # Create a dictionary to store the results for this model configuration\n",
        "        result_i = {\n",
        "            'Model': 'KNN',\n",
        "            'Accuracy': accuracy_mean,\n",
        "            'N_neighbors': i,\n",
        "            'Distance': d,\n",
        "            'Recall': recall_mean,\n",
        "            'Specificity': specificity_mean,\n",
        "            'Precision': precision_mean,\n",
        "            'F1': f1_mean,\n",
        "            'Time': time_mean\n",
        "        }\n",
        "\n",
        "        # Append the results to the DataFrame\n",
        "        df_results = pd.concat([df_results, pd.DataFrame([result_i])], ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9MAIYXuitmw"
      },
      "outputs": [],
      "source": [
        "K = 61                        # Number of neighbors\n",
        "DISTANCE_METRIC = 'manhattan' # Distance metric for KNN\n",
        "LABELS = ('corrosion', 'no corrosion')  # Class labels\n",
        "\n",
        "# ========================\n",
        "# Data Preparation\n",
        "# ========================\n",
        "# Prepare training data\n",
        "X_train = data_train.drop(columns=['label_binary', 'n_image', 'label_multi'])\n",
        "y_train = data_train['label_binary']\n",
        "\n",
        "# Prepare test data\n",
        "X_test = data_test.drop(columns=['label_binary', 'n_image', 'label_multi'])\n",
        "y_test = data_test['label_binary']\n",
        "\n",
        "# ========================\n",
        "# Feature Scaling\n",
        "# ========================\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# ========================\n",
        "# Model Training\n",
        "# ========================\n",
        "model = KNeighborsClassifier(\n",
        "    n_neighbors=K,\n",
        "    metric=DISTANCE_METRIC\n",
        ")\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# ========================\n",
        "# Model Evaluation\n",
        "# ========================\n",
        "# Time prediction process\n",
        "start_time = time.time()\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "execution_time = round(time.time() - start_time, 3)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "cm = confusion_matrix(y_test, y_pred, labels=LABELS)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "recall = round(recall_score(y_test, y_pred, average='binary', pos_label='corrosion'), 3)\n",
        "specificity = round(recall_score(y_test, y_pred, average='binary', pos_label='no corrosion'), 3)\n",
        "precision = round(precision_score(y_test, y_pred, average='binary', pos_label='corrosion'), 3)\n",
        "f1 = round(f1_score(y_test, y_pred, average='binary', pos_label='corrosion'), 3)\n",
        "\n",
        "# ========================\n",
        "# Results Display\n",
        "# ========================\n",
        "print(f'\\nModel with {K} neighbors using {DISTANCE_METRIC} distance:')\n",
        "print(f'Confusion Matrix:\\n{cm}')\n",
        "print(f'Accuracy: {accuracy:.3f}')\n",
        "print(f'Recall (Corrosion Detection): {recall}')\n",
        "print(f'Specificity (No Corrosion): {specificity}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'F1 Score: {f1}')\n",
        "print(f'Execution Time: {execution_time} seconds')\n",
        "\n",
        "# ========================\n",
        "# Results Storage\n",
        "# ========================\n",
        "results_columns = ['Model', 'N_neighbors', 'Distance', 'Accuracy',\n",
        "                  'Recall', 'Specificity', 'Precision', 'F1', 'Time']\n",
        "\n",
        "results_data = {\n",
        "    'Model': 'KNN',\n",
        "    'N_neighbors': K,\n",
        "    'Distance': DISTANCE_METRIC,\n",
        "    'Accuracy': accuracy,\n",
        "    'Recall': recall,\n",
        "    'Specificity': specificity,\n",
        "    'Precision': precision,\n",
        "    'F1': f1,\n",
        "    'Time': execution_time\n",
        "}\n",
        "\n",
        "df_final_results = pd.DataFrame([results_data])\n",
        "print('\\nFinal Results DataFrame:')\n",
        "print(df_final_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV-aY4IzVIZJ"
      },
      "source": [
        "## Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define results DataFrame structure\n",
        "results_columns = ['Model', 'Accuracy', 'Recall', 'Specificity',\n",
        "                  'Precision', 'F1', 'Time']\n",
        "df_results = pd.DataFrame(columns=results_columns)\n",
        "\n",
        "# Configuration parameters\n",
        "VIF_THRESHOLD = 10\n",
        "PVAL_THRESHOLD = 0.01\n",
        "MAX_ITERATIONS = 100\n",
        "\n",
        "def perform_feature_selection(X_train, y_train):\n",
        "    \"\"\"\n",
        "    Perform iterative feature selection using p-values and VIF analysis.\n",
        "\n",
        "    Args:\n",
        "        X_train: Training features DataFrame\n",
        "        y_train: Training labels Series\n",
        "\n",
        "    Returns:\n",
        "        Tuple: (Final model, List of selected features)\n",
        "    \"\"\"\n",
        "    current_features = X_train.columns.tolist()\n",
        "    removed_features = []\n",
        "\n",
        "    # First phase: Remove features with high p-values\n",
        "    while True:\n",
        "        model = sm.Logit(y_train, X_train[current_features]).fit(\n",
        "            disp=0,\n",
        "            maxiter=MAX_ITERATIONS\n",
        "        )\n",
        "\n",
        "        # Get highest p-value feature\n",
        "        p_values = model.pvalues\n",
        "        max_p_feature = p_values.idxmax()\n",
        "        max_p_value = p_values.max()\n",
        "\n",
        "        if max_p_value < PVAL_THRESHOLD:\n",
        "            break\n",
        "\n",
        "        print(f\"Removing feature: {max_p_feature} (p-value: {max_p_value:.3f})\")\n",
        "        current_features.remove(max_p_feature)\n",
        "\n",
        "    # Second phase: Remove features with high multicollinearity\n",
        "    while True:\n",
        "        # Calculate VIF for remaining features\n",
        "        vif_data = pd.DataFrame()\n",
        "        vif_data['Feature'] = current_features\n",
        "        vif_data['VIF'] = [variance_inflation_factor(X_train[current_features].values, i)\n",
        "                          for i in range(len(current_features))]\n",
        "\n",
        "        max_vif = vif_data['VIF'].max()\n",
        "        max_vif_feature = vif_data.loc[vif_data['VIF'].idxmax(), 'Feature']\n",
        "\n",
        "        if max_vif < VIF_THRESHOLD:\n",
        "            print(\"All VIF values below threshold\")\n",
        "            break\n",
        "\n",
        "        print(f\"Removing feature: {max_vif_feature} (VIF: {max_vif:.1f})\")\n",
        "        current_features.remove(max_vif_feature)\n",
        "\n",
        "    return model, current_features\n",
        "\n",
        "# Main cross-validation loop\n",
        "for fold_idx in tqdm(range(5), desc=\"Processing folds\"):\n",
        "    # Split data into train/test sets\n",
        "    test_data = pd.concat([D[fold_idx]])\n",
        "    train_data = pd.concat([d for i, d in enumerate(D) if i != fold_idx])\n",
        "\n",
        "    # Prepare datasets\n",
        "    y_test = test_data['label_binary']\n",
        "    X_test = test_data.drop(columns=['label_binary', 'n_image', 'label_multi'])\n",
        "\n",
        "    y_train = train_data['label_binary']\n",
        "    X_train = train_data.drop(columns=['label_binary', 'n_image', 'label_multi'])\n",
        "\n",
        "    # Encode labels to numerical values\n",
        "    label_mapping = {'no corrosion': 0, 'corrosion': 1}\n",
        "    y_train = y_train.map(label_mapping)\n",
        "    y_test = y_test.map(label_mapping)\n",
        "\n",
        "    # Standardize features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Convert back to DataFrames with original column names\n",
        "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
        "\n",
        "    # Feature selection process\n",
        "    final_model, selected_features = perform_feature_selection(X_train_scaled, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    start_time = time.time()\n",
        "    X_test_final = X_test_scaled[selected_features]\n",
        "    y_pred_proba = final_model.predict(X_test_final)\n",
        "    y_pred = (y_pred_proba >= 0.5).astype(int)\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    # Calculate metrics\n",
        "    cm = confusion_matrix(y_test, y_pred, labels=[1, 0])\n",
        "    print(f\"\\nConfusion matrix for fold {fold_idx+1}:\\n{cm}\")\n",
        "\n",
        "    fold_metrics = {\n",
        "        'accuracy': accuracy_score(y_test, y_pred),\n",
        "        'recall': recall_score(y_test, y_pred, pos_label=1),\n",
        "        'specificity': recall_score(y_test, y_pred, pos_label=0),\n",
        "        'precision': precision_score(y_test, y_pred, pos_label=1),\n",
        "        'f1': f1_score(y_test, y_pred, pos_label=1),\n",
        "        'time': elapsed_time\n",
        "    }\n",
        "\n",
        "    # Store metrics\n",
        "    df_results = pd.concat([\n",
        "        df_results,\n",
        "        pd.DataFrame([{\n",
        "            'Model': 'Logistic Regression',\n",
        "            'Accuracy': fold_metrics['accuracy'],\n",
        "            'Recall': fold_metrics['recall'],\n",
        "            'Specificity': fold_metrics['specificity'],\n",
        "            'Precision': fold_metrics['precision'],\n",
        "            'F1': fold_metrics['f1'],\n",
        "            'Time': fold_metrics['time']\n",
        "        }])\n",
        "    ], ignore_index=True)\n",
        "\n",
        "# Calculate mean metrics across all folds\n",
        "mean_results = {\n",
        "    'Model': 'Logistic Regression',\n",
        "    'Accuracy': df_results['Accuracy'].mean(),\n",
        "    'Recall': df_results['Recall'].mean(),\n",
        "    'Specificity': df_results['Specificity'].mean(),\n",
        "    'Precision': df_results['Precision'].mean(),\n",
        "    'F1': df_results['F1'].mean(),\n",
        "    'Time': df_results['Time'].mean()\n",
        "}\n",
        "\n",
        "# Add mean results to DataFrame\n",
        "df_results = pd.concat([\n",
        "    df_results,\n",
        "    pd.DataFrame([mean_results])\n",
        "], ignore_index=True)\n",
        "\n",
        "print(\"\\nFinal Results:\")\n",
        "print(df_results.round(3))"
      ],
      "metadata": {
        "id": "UeDkA1UPQ_CR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2Ll562aGkft"
      },
      "source": [
        "## SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFL-q8fZGzCJ"
      },
      "outputs": [],
      "source": [
        "# Define the columns for the results DataFrame\n",
        "columns = ['Model', 'C', 'Kernel', 'Accuracy', 'Recall', 'Specificity', 'Precision', 'F1', 'Time']\n",
        "df_results = pd.DataFrame(columns=columns)\n",
        "\n",
        "# Hyperparameter values for SVM\n",
        "C_values = [0.001, 0.01, 0.1, 1]  # Regularization parameter values\n",
        "kernels = ['linear', 'rbf']  # Kernel types to test\n",
        "\n",
        "# Iterate over each combination of C and kernel\n",
        "for c in C_values:\n",
        "    for kernel in kernels:\n",
        "        # Initialize lists to store metrics across folds\n",
        "        recalls = []\n",
        "        specificities = []\n",
        "        precisions = []\n",
        "        f1s = []\n",
        "        accuracies = []\n",
        "        times = []\n",
        "\n",
        "        # Perform 5-fold cross-validation\n",
        "        for fold in range(5):\n",
        "            # Split data into training and testing sets for this fold\n",
        "            test_data = pd.concat([D[fold]])\n",
        "            y_test = test_data['label_binary']\n",
        "            X_test = test_data.drop(columns=['label_binary', 'n_image', 'label_multi'])\n",
        "\n",
        "            train_data = pd.concat([D[i] for i in range(5) if i != fold], ignore_index=True)\n",
        "            y_train = train_data['label_binary']\n",
        "            X_train = train_data.drop(columns=['label_binary', 'n_image', 'label_multi'])\n",
        "\n",
        "            # Scale the features using StandardScaler\n",
        "            scaler = StandardScaler()\n",
        "            X_train_scaled = scaler.fit_transform(X_train)\n",
        "            X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "            # Train the SVM model with the current hyperparameters\n",
        "            model = SVC(C=c, kernel=kernel, verbose=False)\n",
        "            model.fit(X_train_scaled, y_train)\n",
        "\n",
        "            # Measure prediction time\n",
        "            t0 = time.time()\n",
        "            y_pred = model.predict(X_test_scaled)\n",
        "            t1 = time.time()\n",
        "\n",
        "            # Print model details and confusion matrix\n",
        "            print(f'Model with C={c} and kernel={kernel}:')\n",
        "            labels = ('corrosion', 'no corrosion')\n",
        "            cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
        "            print(f'Confusion matrix for C={c}:\\n{cm}')\n",
        "\n",
        "            # Calculate metrics and store them\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "            accuracies.append(accuracy)\n",
        "\n",
        "            recall = recall_score(y_test, y_pred, average='binary', pos_label='corrosion')\n",
        "            recall = round(recall, 3)\n",
        "            print(f'Recall for C={c}: {recall}')\n",
        "            recalls.append(recall)\n",
        "\n",
        "            specificity = recall_score(y_test, y_pred, average='binary', pos_label='no corrosion')\n",
        "            specificity = round(specificity, 3)\n",
        "            specificities.append(specificity)\n",
        "            print(f'Specificity: {specificity}')\n",
        "\n",
        "            precision = precision_score(y_test, y_pred, average='binary', pos_label='corrosion')\n",
        "            precision = round(precision, 3)\n",
        "            precisions.append(precision)\n",
        "\n",
        "            f1 = f1_score(y_test, y_pred, average='binary', pos_label='corrosion')\n",
        "            f1 = round(f1, 3)\n",
        "            f1s.append(f1)\n",
        "\n",
        "            time_taken = t1 - t0\n",
        "            time_taken = round(time_taken, 3)\n",
        "            times.append(time_taken)\n",
        "            print(f'Execution time: {time_taken} seconds\\n')\n",
        "\n",
        "        # Calculate mean metrics across all folds\n",
        "        mean_recall = np.mean(recalls)\n",
        "        mean_specificity = np.mean(specificities)\n",
        "        mean_precision = np.mean(precisions)\n",
        "        mean_f1 = np.mean(f1s)\n",
        "        mean_accuracy = np.mean(accuracies)\n",
        "        mean_time = np.mean(times)\n",
        "\n",
        "        # Create a dictionary to store the results for this configuration\n",
        "        result_row = {\n",
        "            'Model': 'SVM',\n",
        "            'C': c,\n",
        "            'Kernel': kernel,\n",
        "            'Accuracy': mean_accuracy,\n",
        "            'Recall': mean_recall,\n",
        "            'Specificity': mean_specificity,\n",
        "            'Precision': mean_precision,\n",
        "            'F1': mean_f1,\n",
        "            'Time': mean_time\n",
        "        }\n",
        "\n",
        "        # Append the results to the DataFrame\n",
        "        df_results = pd.concat([df_results, pd.DataFrame([result_row])], ignore_index=True)\n",
        "\n",
        "# Display final results summary\n",
        "print(\"\\nFinal Results:\")\n",
        "print(df_results.round(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "5Nu14agRjVg0",
        "outputId": "1dc2c788-e30f-4f54-b54d-a48253ca2b2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LibSVM]Modelo con 0.1 C y con kernelrbf:\n",
            "Matriz de confusion para 0.1 estimadores: \n",
            " [[18477  4450]\n",
            " [18153 49032]]\n",
            "Recall para 0.1 estimadores: 0.806\n",
            "Tiempo de ejecucin: 1045.665 segundos\n",
            "Especificidad: 0.73\n",
            "Precision: 0.504\n",
            "F1: 0.62\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-49-27653a3de822>:51: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  df_resultados=pd.concat([df_resultados, pd.DataFrame([resultado_i])], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Modelo    C Kernel  Accuracy  Recall  Especificidad  Precision    F1  \\\n",
              "0    SVM  0.1    rbf  0.749168   0.806           0.73      0.504  0.62   \n",
              "\n",
              "     Tiempo  \n",
              "0  1045.665  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-85bf6710-6b11-4892-b740-90faff563c94\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Modelo</th>\n",
              "      <th>C</th>\n",
              "      <th>Kernel</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Especificidad</th>\n",
              "      <th>Precision</th>\n",
              "      <th>F1</th>\n",
              "      <th>Tiempo</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SVM</td>\n",
              "      <td>0.1</td>\n",
              "      <td>rbf</td>\n",
              "      <td>0.749168</td>\n",
              "      <td>0.806</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.504</td>\n",
              "      <td>0.62</td>\n",
              "      <td>1045.665</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-85bf6710-6b11-4892-b740-90faff563c94')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-85bf6710-6b11-4892-b740-90faff563c94 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-85bf6710-6b11-4892-b740-90faff563c94');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_a766f9a6-ba37-4075-a76e-c7e86cef1db0\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_resultados')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_a766f9a6-ba37-4075-a76e-c7e86cef1db0 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_resultados');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_resultados",
              "summary": "{\n  \"name\": \"df_resultados\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"Modelo\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"SVM\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"C\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.1,\n        \"max\": 0.1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Kernel\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"rbf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.7491677024147727,\n        \"max\": 0.7491677024147727,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.7491677024147727\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.806,\n        \"max\": 0.806,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.806\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Especificidad\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.73,\n        \"max\": 0.73,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.73\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.504,\n        \"max\": 0.504,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.504\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.62,\n        \"max\": 0.62,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.62\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Tiempo\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1045.665,\n        \"max\": 1045.665,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1045.665\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "C_VALUE = 0.1                # Regularization parameter\n",
        "KERNEL_TYPE = 'rbf'          # Kernel type for SVM\n",
        "LABEL_MAPPING = {            # Label encoding dictionary\n",
        "    'no corrosion': 0,\n",
        "    'corrosion': 1\n",
        "}\n",
        "\n",
        "# ========================\n",
        "# Data Preparation\n",
        "# ========================\n",
        "# Separate features and labels for training data\n",
        "X_train = data_train.drop(columns=['label_binary', 'label_multi', 'n_image'])\n",
        "y_train = data_train['label_binary']\n",
        "\n",
        "# Separate features and labels for test data\n",
        "X_test = data_test.drop(columns=['label_binary', 'label_multi', 'n_image'])\n",
        "y_test = data_test['label_binary']\n",
        "\n",
        "# ========================\n",
        "# Feature Scaling\n",
        "# ========================\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# ========================\n",
        "# Label Encoding\n",
        "# ========================\n",
        "y_train_encoded = y_train.map(LABEL_MAPPING)\n",
        "y_test_encoded = y_test.map(LABEL_MAPPING)\n",
        "\n",
        "# ========================\n",
        "# Model Training\n",
        "# ========================\n",
        "model = SVC(C=C_VALUE, kernel=KERNEL_TYPE, verbose=True)\n",
        "model.fit(X_train_scaled, y_train_encoded)\n",
        "\n",
        "# ========================\n",
        "# Model Evaluation\n",
        "# ========================\n",
        "# Time prediction only\n",
        "start_time = time.time()\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "execution_time = round(time.time() - start_time, 3)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "cm = confusion_matrix(y_test_encoded, y_pred, labels=[1, 0])\n",
        "accuracy = accuracy_score(y_test_encoded, y_pred)\n",
        "recall = round(recall_score(y_test_encoded, y_pred, pos_label=1), 3)\n",
        "specificity = round(recall_score(y_test_encoded, y_pred, pos_label=0), 3)\n",
        "precision = round(precision_score(y_test_encoded, y_pred, pos_label=1), 3)\n",
        "f1 = round(f1_score(y_test_encoded, y_pred, pos_label=1), 3)\n",
        "\n",
        "# ========================\n",
        "# Results Display\n",
        "# ========================\n",
        "print(f'\\nModel with C={C_VALUE} and {KERNEL_TYPE} kernel:')\n",
        "print(f'Confusion Matrix:\\n{cm}')\n",
        "print(f'Accuracy: {accuracy:.3f}')\n",
        "print(f'Recall (Corrosion): {recall}')\n",
        "print(f'Specificity (No Corrosion): {specificity}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'F1 Score: {f1}')\n",
        "print(f'Execution Time: {execution_time} seconds')\n",
        "\n",
        "# ========================\n",
        "# Results Storage\n",
        "# ========================\n",
        "results_columns = ['Model', 'C', 'Kernel', 'Accuracy',\n",
        "                  'Recall', 'Specificity', 'Precision', 'F1', 'Time']\n",
        "results_data = {\n",
        "    'Model': 'SVM',\n",
        "    'C': C_VALUE,\n",
        "    'Kernel': KERNEL_TYPE,\n",
        "    'Accuracy': accuracy,\n",
        "    'Recall': recall,\n",
        "    'Specificity': specificity,\n",
        "    'Precision': precision,\n",
        "    'F1': f1,\n",
        "    'Time': execution_time\n",
        "}\n",
        "\n",
        "df_results = pd.DataFrame([results_data])\n",
        "print('\\nResults DataFrame:')\n",
        "print(df_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW1ZLrK_H0Fw"
      },
      "source": [
        "# WRAPPER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqJ3MBpjiM8O"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlly0KqmOicQ"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hybADwPeiNLI"
      },
      "outputs": [],
      "source": [
        " =============================================\n",
        "# Model Configuration & Feature Selection Setup\n",
        "# =============================================\n",
        "# Initialize LightGBM classifier with binary classification settings\n",
        "lgbm_model = LGBMClassifier(\n",
        "    objective='binary',        # Binary classification task\n",
        "    boosting_type='gbdt',      # Gradient Boosting Decision Tree\n",
        "    learning_rate=0.05,        # Shrinkage rate for updates\n",
        "    num_leaves=10,             # Maximum number of leaves per tree\n",
        "    n_estimators=50,           # Number of boosting rounds\n",
        "    random_state=42,           # Seed for reproducibility\n",
        "    n_jobs=-1,                 # Use all available cores\n",
        "    eval_metric='logloss',     # Evaluation metric during training\n",
        "    verbose=-1                 # Silence output\n",
        ")\n",
        "\n",
        "# Configure sequential feature selection\n",
        "feature_selector = SFS(\n",
        "    estimator=lgbm_model,\n",
        "    k_features=(5, 25),        # Target feature range (min, max)\n",
        "    forward=True,               # Forward selection approach\n",
        "    floating=False,             # No floating selection\n",
        "    scoring='f1',               # Optimization metric\n",
        "    cv=5,                       # 5-fold cross-validation\n",
        "    n_jobs=-1,                  # Use all available cores\n",
        "    verbose=2                   # Medium verbosity\n",
        ")\n",
        "\n",
        "# ======================\n",
        "# Data Preparation\n",
        "# ======================\n",
        "# Prepare training data (exclude non-feature columns)\n",
        "X_train = data_train.drop(columns=['label_binary', 'n_image', 'label_multi'])\n",
        "y_train = data_train['label_binary']\n",
        "\n",
        "# Encode target labels (0: no corrosion, 1: corrosion)\n",
        "label_mapping = {'no corrosion': 0, 'corrosion': 1}\n",
        "y_train_encoded = y_train.map(label_mapping)\n",
        "\n",
        "# ======================\n",
        "# Feature Selection\n",
        "# ======================\n",
        "# Perform feature selection using training data\n",
        "feature_selector.fit(X_train, y_train_encoded)\n",
        "\n",
        "# ======================\n",
        "# Results Extraction\n",
        "# ======================\n",
        "# Get selected feature names and convert to list\n",
        "selected_features = list(feature_selector.k_feature_names_)\n",
        "print(\"Selected features:\", selected_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gSSvO9ZFiwQ"
      },
      "outputs": [],
      "source": [
        "wrapper_features=['label_binary', 'n_image']+selected_features\n",
        "data_train_wrapper=data_train[wrapper_features]\n",
        "data_test_wrapper=data_test[wrapper_features]\n",
        "data_train_wrapper_kf=data_train_wrapper.copy()\n",
        "X1, X2, X3, X4, X5 = create_kfold_sets(data_train=data_train_wrapper_kf)\n",
        "D = [X1, X2, X3, X4, X5]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest"
      ],
      "metadata": {
        "id": "yEvJru-VsNhd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtOXZJ9psFzp"
      },
      "outputs": [],
      "source": [
        "# Define the columns for the results DataFrame\n",
        "columns = ['Model', 'N_estimators', 'Criterion', 'Min_samples_leaf',\n",
        "           'Accuracy',\n",
        "           'Recall',\n",
        "           'Specificity',\n",
        "           'Precision',\n",
        "           'F1',\n",
        "           'Time']\n",
        "df_results = pd.DataFrame(columns=columns)\n",
        "\n",
        "# Define parameters for Grid Search\n",
        "n_estimators = [25, 50, 75, 100]\n",
        "criterion = ['gini', 'entropy']\n",
        "min_samples_leaf = [1, 2, 4, 8, 16, 32]\n",
        "\n",
        "for i in n_estimators:\n",
        "    for k in criterion:\n",
        "        for d in min_samples_leaf:\n",
        "            recalls = []\n",
        "            precisions = []\n",
        "            f1s = []\n",
        "            accuracies = []\n",
        "            specificities = []\n",
        "            times = []\n",
        "            for j in range(5):\n",
        "                # Prepare test and train sets for this fold\n",
        "                d_test = pd.concat([D[j]])\n",
        "                y_test = d_test['label_binary']\n",
        "                X_test = d_test.drop(columns=['label_binary', 'n_image', 'label_multi'])\n",
        "                d_train = pd.concat([D[k] for k in range(5) if k != j], ignore_index=True)\n",
        "                y_train = d_train['label_binary']\n",
        "                X_train = d_train.drop(columns=['label_binary', 'n_image', 'label_multi'])\n",
        "\n",
        "                # Initialize and fit the model\n",
        "                model = RandomForestClassifier(\n",
        "                    n_estimators=i,\n",
        "                    criterion=k,\n",
        "                    min_samples_leaf=d,\n",
        "                    bootstrap=True,\n",
        "                    random_state=42)\n",
        "                model.fit(X_train, y_train)\n",
        "\n",
        "                # Measure execution time\n",
        "                t0 = time.time()\n",
        "                y_pred = model.predict(X_test)\n",
        "                print(f'Model with {i} estimators:')\n",
        "                labels = ('corrosion', 'no corrosion')\n",
        "                cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
        "                print(f'Confusion matrix for {i} estimators: \\n {cm}')\n",
        "\n",
        "                # Calculate metrics\n",
        "                accuracy = accuracy_score(y_test, y_pred)\n",
        "                accuracies.append(accuracy)\n",
        "                recall = recall_score(y_test, y_pred, average='binary', pos_label='corrosion')\n",
        "                recall = round(recall, 3)\n",
        "                print(f'Recall for {i} estimators: {recall}')\n",
        "                recalls.append(recall)\n",
        "                specificity = recall_score(y_test, y_pred, average='binary', pos_label='no corrosion')\n",
        "                specificity = round(specificity, 3)\n",
        "                specificities.append(specificity)\n",
        "                print(f'Specificity: {specificity}')\n",
        "                precision = precision_score(y_test, y_pred, average='binary', pos_label='corrosion')\n",
        "                precision = round(precision, 3)\n",
        "                precisions.append(precision)\n",
        "                f1 = f1_score(y_test, y_pred, average='binary', pos_label='corrosion')\n",
        "                f1 = round(f1, 3)\n",
        "                f1s.append(f1)\n",
        "                t1 = time.time()\n",
        "                time_taken = t1 - t0\n",
        "                time_taken = round(time_taken, 3)\n",
        "                times.append(time_taken)\n",
        "                print(f'Execution time: {time_taken} seconds')\n",
        "                print('\\n')\n",
        "\n",
        "            # Calculate mean metrics across folds\n",
        "            recall_mean = np.mean(recalls)\n",
        "            specificity_mean = np.mean(specificities)\n",
        "            precision_mean = np.mean(precisions)\n",
        "            f1_mean = np.mean(f1s)\n",
        "            accuracy_mean = np.mean(accuracies)\n",
        "            time_mean = np.mean(times)\n",
        "\n",
        "            # Append results to DataFrame\n",
        "            result_i = {'Model': 'Random Forest', 'Accuracy': accuracy_mean, 'N_estimators': i, 'Criterion': k, 'Min_samples_leaf': d,\n",
        "                        'Recall': recall_mean, 'Specificity': specificity_mean, 'Precision': precision_mean, 'F1': f1_mean,\n",
        "                        'Time': time_mean}\n",
        "            df_results = pd.concat([df_results, pd.DataFrame([result_i])], ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fzd01lUNsFzq"
      },
      "outputs": [],
      "source": [
        "df_resultados.sort_values(by='Recall', ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiiByLlwsFzr"
      },
      "source": [
        "Chosen model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvwxlnwQsFzs"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "min_samples = 1\n",
        "n_estimators = 50\n",
        "criterion = 'entropy'\n",
        "\n",
        "# Assuming data_train and data_test are already defined\n",
        "X_train = data_train.drop(columns=['label_binary', 'n_image', 'label_multi'])\n",
        "y_train = data_train['label_binary']\n",
        "X_test = data_test.drop(columns=['label_binary', 'n_image', 'label_multi'])\n",
        "y_test = data_test['label_binary']\n",
        "\n",
        "# Initialize and train the model\n",
        "model = RandomForestClassifier(\n",
        "    n_estimators=n_estimators,\n",
        "    criterion=criterion,\n",
        "    min_samples_leaf=min_samples,\n",
        "    bootstrap=True,\n",
        "    random_state=42\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Measure execution time\n",
        "t0 = time.time()\n",
        "y_pred = model.predict(X_test)\n",
        "t1 = time.time()\n",
        "\n",
        "# Metrics calculation\n",
        "labels = ('corrosion', 'no corrosion')\n",
        "cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
        "print(f'Model with {n_estimators} estimators:')\n",
        "print(f'Confusion matrix for {n_estimators} estimators: \\n {cm}')\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy for {n_estimators} estimators: {accuracy}')\n",
        "recall = recall_score(y_test, y_pred, average='binary', pos_label='corrosion')\n",
        "recall = round(recall, 3)\n",
        "print(f'Recall for {n_estimators} estimators: {recall}')\n",
        "specificity = recall_score(y_test, y_pred, average='binary', pos_label='no corrosion')\n",
        "specificity = round(specificity, 3)\n",
        "print(f'Specificity: {specificity}')\n",
        "precision = precision_score(y_test, y_pred, average='binary', pos_label='corrosion')\n",
        "precision = round(precision, 3)\n",
        "print(f'Precision for {n_estimators} estimators: {precision}')\n",
        "f1 = f1_score(y_test, y_pred, average='binary', pos_label='corrosion')\n",
        "f1 = round(f1, 3)\n",
        "print(f'F1 for {n_estimators} estimators: {f1}')\n",
        "time_taken = t1 - t0\n",
        "time_taken = round(time_taken, 3)\n",
        "print(f'Execution time: {time_taken} seconds')\n",
        "\n",
        "# Create a DataFrame to store results\n",
        "columns = ['Model', 'N_estimators', 'Criterion', 'Min_samples_leaf',\n",
        "           'Accuracy',\n",
        "           'Recall',\n",
        "           'Specificity',\n",
        "           'Precision',\n",
        "           'F1',\n",
        "           'Time']\n",
        "df_results = pd.DataFrame(columns=columns)\n",
        "\n",
        "# Append results to DataFrame\n",
        "result_rf = {'Model': 'Random Forest', 'Accuracy': accuracy, 'N_estimators': n_estimators, 'Criterion': criterion, 'Min_samples_leaf': min_samples,\n",
        "             'Recall': recall, 'Specificity': specificity, 'Precision': precision, 'F1': f1,\n",
        "             'Time': time_taken}\n",
        "df_results = pd.concat([df_results, pd.DataFrame([result_rf])], ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pK_veW2TsFzs"
      },
      "outputs": [],
      "source": [
        "# Get feature importances from the trained model\n",
        "importancias = model.feature_importances_\n",
        "\n",
        "# Plot feature importances as a bar chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(X_train.columns, importancias, color='skyblue')\n",
        "plt.xticks(rotation=90, fontsize=10)\n",
        "plt.xlabel('Features', fontsize=12)\n",
        "plt.ylabel('Importance', fontsize=12)\n",
        "plt.title('Feature Importances from Random Forest', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrQAltwlsFzt"
      },
      "outputs": [],
      "source": [
        "# Create a DataFrame from feature importances\n",
        "importancias_df = pd.DataFrame(importancias, index=X_train.columns, columns=['Importance'])\n",
        "\n",
        "# Sort the DataFrame by importance in descending order\n",
        "sorted_importances_df = importancias_df.sort_values(by='Importance', ascending=False)\n",
        "print(sorted_importances_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIIqqo3EsFzu"
      },
      "source": [
        "## XGBOOST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_ryGiC5sFzu"
      },
      "outputs": [],
      "source": [
        "# Define the columns for the results DataFrame\n",
        "columns = ['Model', 'N_estimators', 'Learning_rate', 'Min_child_weight',\n",
        "           'Accuracy',\n",
        "           'Recall',\n",
        "           'Specificity',\n",
        "           'Precision',\n",
        "           'F1',\n",
        "           'Time']\n",
        "df_results = pd.DataFrame(columns=columns)\n",
        "\n",
        "# Define parameters for Grid Search\n",
        "n_estimators = [25, 50, 75, 100, 125, 150]\n",
        "metric = ['logloss']\n",
        "learning_rates = [0.01, 0.05, 0.1, 0.5]\n",
        "min_child_weight = [1, 2, 4, 8, 16, 32]\n",
        "\n",
        "for i in n_estimators:\n",
        "    for d in learning_rates:\n",
        "        for k in min_child_weight:\n",
        "            for m in metric:\n",
        "                recalls = []\n",
        "                specificities = []\n",
        "                precisions = []\n",
        "                f1s = []\n",
        "                accuracies = []\n",
        "                times = []\n",
        "                for j in range(5):\n",
        "                    d_test = pd.concat([D[j]])\n",
        "                    y_test = d_test['label_binary']\n",
        "                    X_test = d_test.drop(columns=['label_binary', 'n_image', 'label_multi'])\n",
        "                    d_train = pd.concat([D[k] for k in range(5) if k != j], ignore_index=True)\n",
        "                    y_train = d_train['label_binary']\n",
        "                    X_train = d_train.drop(columns=['label_binary', 'n_image', 'label_multi'])\n",
        "                    mapping = {'no corrosion': 0, 'corrosion': 1}\n",
        "                    y_train = y_train.map(mapping)\n",
        "                    y_test = y_test.map(mapping)\n",
        "\n",
        "                    # Initialize and train the XGBoost model\n",
        "                    model = xgb(\n",
        "                        objective='binary:logistic',  # Binary classification\n",
        "                        n_estimators=i,             # Number of trees (boosting rounds)\n",
        "                        seed=42,                       # For reproducibility\n",
        "                        learning_rate=d,\n",
        "                        min_child_weight=k,\n",
        "                        eval_metric=m\n",
        "                    )\n",
        "                    model.fit(X_train, y_train)\n",
        "\n",
        "                    # Measure execution time\n",
        "                    t0 = time.time()\n",
        "                    y_pred = model.predict(X_test)\n",
        "                    print(f'Model with {i} estimators, {d} learning rate, and {k} min child weight:')\n",
        "                    labels = (1, 0)\n",
        "                    cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
        "                    print(f'Confusion matrix for {i} estimators: \\n {cm}')\n",
        "\n",
        "                    # Calculate metrics\n",
        "                    accuracy = accuracy_score(y_test, y_pred)\n",
        "                    accuracies.append(accuracy)\n",
        "                    recall = recall_score(y_test, y_pred, average='binary')\n",
        "                    recall = round(recall, 3)\n",
        "                    print(f'Recall for {i} estimators: {recall}')\n",
        "                    recalls.append(recall)\n",
        "                    specificity = recall_score(y_test, y_pred, average='binary', pos_label=0)\n",
        "                    specificity = round(specificity, 3)\n",
        "                    specificities.append(specificity)\n",
        "                    print(f'Specificity: {specificity}')\n",
        "                    precision = precision_score(y_test, y_pred, average='binary')\n",
        "                    precision = round(precision, 3)\n",
        "                    precisions.append(precision)\n",
        "                    f1 = f1_score(y_test, y_pred, average='binary')\n",
        "                    f1 = round(f1, 3)\n",
        "                    f1s.append(f1)\n",
        "                    t1 = time.time()\n",
        "                    time_taken = t1 - t0\n",
        "                    time_taken = round(time_taken, 3)\n",
        "                    times.append(time_taken)\n",
        "                    print(f'Execution time: {time_taken} seconds')\n",
        "                    print('\\n')\n",
        "\n",
        "                # Calculate mean metrics across folds\n",
        "                recall_mean = np.mean(recalls)\n",
        "                specificity_mean = np.mean(specificities)\n",
        "                precision_mean = np.mean(precisions)\n",
        "                f1_mean = np.mean(f1s)\n",
        "                accuracy_mean = np.mean(accuracies)\n",
        "                time_mean = np.mean(times)\n",
        "\n",
        "                # Append results to DataFrame\n",
        "                result_i = {'Model': 'XGBoost', 'Accuracy': accuracy_mean, 'N_estimators': i, 'Learning_rate': d, 'Min_child_weight': k,\n",
        "                            'Recall': recall_mean, 'Specificity': specificity_mean,\n",
        "                            'Precision': precision_mean, 'F1': f1_mean,\n",
        "                            'Time': time_mean}\n",
        "                df_results = pd.concat([df_results, pd.DataFrame([result_i])], ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Prw4v6ThsFzv"
      },
      "outputs": [],
      "source": [
        "df_results.sort_values(by='Recall', ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_LsTcn8sFzv"
      },
      "source": [
        "Chosen model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-qfACKgsFzw"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "n_estimators = 75\n",
        "learning_rate = 0.01\n",
        "min_child_weight = 32\n",
        "\n",
        "# Initialize and train the XGBoost model\n",
        "model = xgb(\n",
        "    objective='binary:logistic',  # Binary classification\n",
        "    n_estimators=n_estimators,     # Number of trees (boosting rounds)\n",
        "    seed=42,                       # For reproducibility\n",
        "    learning_rate=learning_rate,\n",
        "    min_child_weight=min_child_weight,\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "\n",
        "# Prepare data\n",
        "X_train = data_train.drop(columns=['label_binary', 'n_image', 'label_multi'])\n",
        "y_train = data_train['label_binary']\n",
        "X_test = data_test.drop(columns=['label_binary', 'n_image', 'label_multi'])\n",
        "y_test = data_test['label_binary']\n",
        "\n",
        "# Map labels to numeric values\n",
        "label_mapping = {'no corrosion': 0, 'corrosion': 1}\n",
        "y_train = y_train.map(label_mapping)\n",
        "y_test = y_test.map(label_mapping)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Measure execution time\n",
        "t0 = time.time()\n",
        "y_pred = model.predict(X_test)\n",
        "t1 = time.time()\n",
        "\n",
        "# Print model details and metrics\n",
        "print(f'Model with {n_estimators} estimators, {learning_rate} learning rate, and {min_child_weight} min child weight:')\n",
        "labels = (1, 0)\n",
        "cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
        "print(f'Confusion matrix for {n_estimators} estimators: \\n {cm}')\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy for {n_estimators} estimators: {accuracy}')\n",
        "recall = recall_score(y_test, y_pred, average='binary')\n",
        "recall = round(recall, 3)\n",
        "print(f'Recall for {n_estimators} estimators: {recall}')\n",
        "specificity = recall_score(y_test, y_pred, average='binary', pos_label=0)\n",
        "specificity = round(specificity, 3)\n",
        "print(f'Specificity: {specificity}')\n",
        "precision = precision_score(y_test, y_pred, average='binary')\n",
        "precision = round(precision, 3)\n",
        "print(f'Precision for {n_estimators} estimators: {precision}')\n",
        "f1 = f1_score(y_test, y_pred, average='binary')\n",
        "f1 = round(f1, 3)\n",
        "print(f'F1 for {n_estimators} estimators: {f1}')\n",
        "time_taken = t1 - t0\n",
        "time_taken = round(time_taken, 3)\n",
        "print(f'Execution time: {time_taken} seconds')\n",
        "\n",
        "# Create a DataFrame to store results\n",
        "columns = ['Model', 'N_estimators', 'Learning_rate', 'Min_child_weight', 'Accuracy',\n",
        "           'Recall',\n",
        "           'Specificity',\n",
        "           'Precision',\n",
        "           'F1',\n",
        "           'Time']\n",
        "df_results = pd.DataFrame(columns=columns)\n",
        "\n",
        "# Append results to DataFrame\n",
        "result_xgb = {'Model': 'XGBoost', 'Accuracy': accuracy, 'N_estimators': n_estimators, 'Learning_rate': learning_rate, 'Min_child_weight': min_child_weight,\n",
        "              'Recall': recall, 'Specificity': specificity, 'Precision': precision, 'F1': f1,\n",
        "              'Time': time_taken}\n",
        "df_results = pd.concat([df_results, pd.DataFrame([result_xgb])], ignore_index=True)\n",
        "print(df_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwKIYK5SsFzx"
      },
      "source": [
        "## LIGHTGBM GBDT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEue1kX1sFzx"
      },
      "outputs": [],
      "source": [
        "# Define the columns for the results DataFrame\n",
        "columns = ['Model', 'N_estimators', 'Learning_rate', 'N_bins', 'Num_leaves', 'Accuracy',\n",
        "           'Metric',\n",
        "           'Recall',\n",
        "           'Specificity',\n",
        "           'Precision',\n",
        "           'F1',\n",
        "           'Time']\n",
        "df_results = pd.DataFrame(columns=columns)\n",
        "\n",
        "# Define parameters for Grid Search\n",
        "n_estimators = [25, 50, 75, 100, 125, 150]\n",
        "metric = ['logloss']\n",
        "learning_rates = [0.025, 0.05, 0.1, 0.2, 0.4]\n",
        "n_bins = [255]\n",
        "num_leaves = [10, 30, 50]\n",
        "\n",
        "for i in n_estimators:\n",
        "    for d in learning_rates:\n",
        "        for k in n_bins:\n",
        "            for l in num_leaves:\n",
        "                for m in metric:\n",
        "                    recalls = []\n",
        "                    specificities = []\n",
        "                    precisions = []\n",
        "                    f1s = []\n",
        "                    accuracies = []\n",
        "                    times = []\n",
        "                    for j in range(5):\n",
        "                        # Prepare test and train sets for this fold\n",
        "                        d_test = pd.concat([D[j]])\n",
        "                        y_test = d_test['label_binary']\n",
        "                        X_test = d_test.drop(columns=['label_binary', 'n_image', 'label_multi'])\n",
        "                        d_train = pd.concat([D[k] for k in range(5) if k != j], ignore_index=True)\n",
        "                        y_train = d_train['label_binary']\n",
        "                        X_train = d_train.drop(columns=['label_binary', 'n_image', 'label_multi'])\n",
        "                        label_mapping = {'no corrosion': 0, 'corrosion': 1}\n",
        "                        y_train = y_train.map(label_mapping)\n",
        "                        y_test = y_test.map(label_mapping)\n",
        "\n",
        "                        # Initialize and train the LightGBM model\n",
        "                        model = lgbm(\n",
        "                            objective='binary',  # Binary classification\n",
        "                            boosting_type='gbdt',  # Boosting type\n",
        "                            n_estimators=i,             # Number of trees (boosting rounds)\n",
        "                            seed=42,                       # For reproducibility\n",
        "                            learning_rate=d,\n",
        "                            n_bins=k,\n",
        "                            num_leaves=l,\n",
        "                            eval_metric=m,\n",
        "                            verbose=-1\n",
        "                        )\n",
        "                        model.fit(X_train, y_train)\n",
        "\n",
        "                        # Measure execution time\n",
        "                        t0 = time.time()\n",
        "                        y_pred = model.predict(X_test)\n",
        "                        print(f'Model with {i} estimators, {d} learning rate, and {l} leaves:')\n",
        "                        labels = (1, 0)\n",
        "                        cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
        "                        print(f'Confusion matrix for {i} estimators: \\n {cm}')\n",
        "\n",
        "                        # Calculate metrics\n",
        "                        accuracy = accuracy_score(y_test, y_pred)\n",
        "                        accuracies.append(accuracy)\n",
        "                        recall = recall_score(y_test, y_pred, average='binary')\n",
        "                        recall = round(recall, 3)\n",
        "                        print(f'Recall for {i} estimators: {recall}')\n",
        "                        recalls.append(recall)\n",
        "                        specificity = recall_score(y_test, y_pred, average='binary', pos_label=0)\n",
        "                        specificity = round(specificity, 3)\n",
        "                        specificities.append(specificity)\n",
        "                        print(f'Specificity: {specificity}')\n",
        "                        precision = precision_score(y_test, y_pred, average='binary')\n",
        "                        precision = round(precision, 3)\n",
        "                        precisions.append(precision)\n",
        "                        f1 = f1_score(y_test, y_pred, average='binary')\n",
        "                        f1 = round(f1, 3)\n",
        "                        f1s.append(f1)\n",
        "                        t1 = time.time()\n",
        "                        time_taken = t1 - t0\n",
        "                        time_taken = round(time_taken, 3)\n",
        "                        times.append(time_taken)\n",
        "                        print(f'Execution time: {time_taken} seconds')\n",
        "                        print('\\n')\n",
        "\n",
        "                    # Calculate mean metrics across folds\n",
        "                    recall_mean = np.mean(recalls)\n",
        "                    specificity_mean = np.mean(specificities)\n",
        "                    precision_mean = np.mean(precisions)\n",
        "                    f1_mean = np.mean(f1s)\n",
        "                    accuracy_mean = np.mean(accuracies)\n",
        "                    time_mean = np.mean(times)\n",
        "\n",
        "                    # Append results to DataFrame\n",
        "                    result_i = {'Model': 'LGBM', 'Accuracy': accuracy_mean, 'N_estimators': i, 'Learning_rate': d, 'N_bins': k, 'Num_leaves': l,\n",
        "                                'Metric': m,\n",
        "                                'Recall': recall_mean, 'Specificity': specificity_mean,\n",
        "                                'Precision': precision_mean, 'F1': f1_mean,\n",
        "                                'Time': time_mean}\n",
        "                    df_results = pd.concat([df_results, pd.DataFrame([result_i])], ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsW0WMNesFzy"
      },
      "outputs": [],
      "source": [
        "df_resultados.sort_values(by='Recall', ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chosen model"
      ],
      "metadata": {
        "id": "w3VwiNVWsFzz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Y1U0Vp2sFzz"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "n_estimators = 25\n",
        "learning_rate = 0.025\n",
        "n_bins = 255\n",
        "num_leaves = 10\n",
        "\n",
        "# Prepare data\n",
        "X_train = data_train.drop(columns=['label_binary', 'n_image', 'label_multi'])\n",
        "y_train = data_train['label_binary']\n",
        "label_mapping = {'no corrosion': 0, 'corrosion': 1}\n",
        "y_train = y_train.map(label_mapping)\n",
        "X_test = data_test.drop(columns=['label_binary', 'n_image', 'label_multi'])\n",
        "y_test = data_test['label_binary']\n",
        "y_test = y_test.map(label_mapping)\n",
        "\n",
        "# Initialize and train the LightGBM model\n",
        "model = lgbm(\n",
        "    objective='binary',  # Binary classification\n",
        "    boosting_type='gbdt',  # Boosting type\n",
        "    n_estimators=n_estimators,             # Number of trees (boosting rounds)\n",
        "    seed=42,                       # For reproducibility\n",
        "    learning_rate=learning_rate,\n",
        "    eval_metric='logloss',\n",
        "    verbose=-1,\n",
        "    n_bins=n_bins,\n",
        "    num_leaves=num_leaves\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Measure execution time\n",
        "t0 = time.time()\n",
        "y_pred = model.predict(X_test)\n",
        "t1 = time.time()\n",
        "time_taken = t1 - t0\n",
        "time_taken = round(time_taken, 3)\n",
        "\n",
        "# Print model details and metrics\n",
        "print(f'Model with {n_estimators} estimators:')\n",
        "labels = (1, 0)\n",
        "cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
        "print(f'Confusion matrix for estimators: \\n {cm}')\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred, average='binary')\n",
        "recall = round(recall, 3)\n",
        "print(f'Recall for estimators: {recall}')\n",
        "print(f'Execution time: {time_taken} seconds')\n",
        "precision = precision_score(y_test, y_pred, average='binary')\n",
        "precision = round(precision, 3)\n",
        "specificity = recall_score(y_test, y_pred, average='binary', pos_label=0)\n",
        "specificity = round(specificity, 3)\n",
        "f1 = f1_score(y_test, y_pred, average='binary')\n",
        "f1 = round(f1, 3)\n",
        "print(f'Specificity: {specificity}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'F1: {f1}')\n",
        "\n",
        "# Create a DataFrame to store results\n",
        "columns = ['Model', 'N_estimators', 'Accuracy', 'Learning_rate', 'N_bins', 'Num_leaves',\n",
        "           'Metric',\n",
        "           'Recall',\n",
        "           'Specificity',\n",
        "           'Precision',\n",
        "           'F1',\n",
        "           'Time']\n",
        "df_results = pd.DataFrame(columns=columns)\n",
        "\n",
        "# Append results to DataFrame\n",
        "result_i = {'Model': 'LightGBM', 'N_estimators': n_estimators, 'Learning_rate': learning_rate, 'N_bins': n_bins, 'Num_leaves': num_leaves,\n",
        "            'Metric': 'logloss', 'Accuracy': accuracy,\n",
        "            'Recall': recall, 'Specificity': specificity,\n",
        "            'Precision': precision, 'F1': f1,\n",
        "            'Time': time_taken}\n",
        "df_results = pd.concat([df_results, pd.DataFrame([result_i])], ignore_index=True)\n",
        "\n",
        "print(df_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1gGQ6NMsFz0"
      },
      "source": [
        "## LIGHTGBM GOSS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3IByTIpsFz1"
      },
      "outputs": [],
      "source": [
        "# Define the columns for the results DataFrame\n",
        "columns = ['Model', 'Top_rate', 'Other_rate', 'Learning_rate', 'Num_leaves',\n",
        "           'Accuracy',\n",
        "           'Recall',\n",
        "           'Specificity',\n",
        "           'Precision',\n",
        "           'F1',\n",
        "           'Time']\n",
        "df_results = pd.DataFrame(columns=columns)\n",
        "\n",
        "# Define parameters for Grid Search\n",
        "top_rate = [0.2, 0.4, 0.6]\n",
        "other_rate = [0.05, 0.1, 0.3]  # Corrected to be a list\n",
        "learning_rates = [0.025, 0.05, 0.1, 0.2]\n",
        "num_leaves = [10, 30, 50]\n",
        "\n",
        "for i in top_rate:\n",
        "    for d in learning_rates:\n",
        "        for k in other_rate:\n",
        "            for l in num_leaves:\n",
        "                recalls = []\n",
        "                specificities = []\n",
        "                precisions = []\n",
        "                f1s = []\n",
        "                accuracies = []\n",
        "                times = []\n",
        "                for j in range(5):\n",
        "                    # Prepare test and train sets for this fold\n",
        "                    d_test = pd.concat([D[j]])\n",
        "                    y_test = d_test['label_binary']\n",
        "                    X_test = d_test.drop(columns=['label_binary', 'n_image', 'label_multi'])\n",
        "                    d_train = pd.concat([D[k] for k in range(5) if k != j], ignore_index=True)\n",
        "                    y_train = d_train['label_binary']\n",
        "                    X_train = d_train.drop(columns=['label_binary', 'n_image', 'label_multi'])\n",
        "                    label_mapping = {'no corrosion': 0, 'corrosion': 1}\n",
        "                    y_train = y_train.map(label_mapping)\n",
        "                    y_test = y_test.map(label_mapping)\n",
        "\n",
        "                    # Initialize and train the LightGBM model\n",
        "                    model = lgbm(\n",
        "                        objective='binary',  # Binary classification\n",
        "                        boosting_type='goss',  # Boosting type\n",
        "                        top_rate=i,             # Top rate\n",
        "                        seed=42,                       # For reproducibility\n",
        "                        learning_rate=d,\n",
        "                        other_rate=k,\n",
        "                        num_leaves=l,\n",
        "                        verbose=-1\n",
        "                    )\n",
        "                    model.fit(X_train, y_train)\n",
        "\n",
        "                    # Measure execution time\n",
        "                    t0 = time.time()\n",
        "                    y_pred = model.predict(X_test)\n",
        "                    print(f'Model with {i} top rate, {d} learning rate, and {l} leaves:')\n",
        "                    labels = (1, 0)\n",
        "                    cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
        "                    print(f'Confusion matrix for {i} top rate: \\n {cm}')\n",
        "\n",
        "                    # Calculate metrics\n",
        "                    accuracy = accuracy_score(y_test, y_pred)\n",
        "                    accuracies.append(accuracy)\n",
        "                    recall = recall_score(y_test, y_pred, average='binary')\n",
        "                    recall = round(recall, 3)\n",
        "                    print(f'Recall for {i} top rate: {recall}')\n",
        "                    recalls.append(recall)\n",
        "                    specificity = recall_score(y_test, y_pred, average='binary', pos_label=0)\n",
        "                    specificity = round(specificity, 3)\n",
        "                    specificities.append(specificity)\n",
        "                    print(f'Specificity: {specificity}')\n",
        "                    precision = precision_score(y_test, y_pred, average='binary')\n",
        "                    precision = round(precision, 3)\n",
        "                    precisions.append(precision)\n",
        "                    f1 = f1_score(y_test, y_pred, average='binary')\n",
        "                    f1 = round(f1, 3)\n",
        "                    f1s.append(f1)\n",
        "                    t1 = time.time()\n",
        "                    time_taken = t1 - t0\n",
        "                    time_taken = round(time_taken, 3)\n",
        "                    times.append(time_taken)\n",
        "                    print(f'Execution time: {time_taken} seconds')\n",
        "                    print('\\n')\n",
        "\n",
        "                # Calculate mean metrics across folds\n",
        "                recall_mean = np.mean(recalls)\n",
        "                specificity_mean = np.mean(specificities)\n",
        "                precision_mean = np.mean(precisions)\n",
        "                f1_mean = np.mean(f1s)\n",
        "                accuracy_mean = np.mean(accuracies)\n",
        "                time_mean = np.mean(times)\n",
        "\n",
        "                # Append results to DataFrame\n",
        "                result_i = {'Model': 'LightGBM', 'Accuracy': accuracy_mean, 'Top_rate': i, 'Learning_rate': d, 'Other_rate': k, 'Num_leaves': l,\n",
        "                            'Recall': recall_mean, 'Specificity': specificity_mean,\n",
        "                            'Precision': precision_mean, 'F1': f1_mean,\n",
        "                            'Time': time_mean}\n",
        "                df_results = pd.concat([df_results, pd.DataFrame([result_i])], ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2V-BWAubsFz1"
      },
      "outputs": [],
      "source": [
        "df_resultados.sort_values(by='Recall', ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chosen model"
      ],
      "metadata": {
        "id": "_LEcFFResFz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model parameters\n",
        "top_rate = 0.2\n",
        "learning_rate = 0.05\n",
        "other_rate = 0.05\n",
        "num_leaves = 10\n",
        "\n",
        "# Prepare training and testing data\n",
        "X_train = data_train.drop(columns=['label_binary', 'label_multi', 'n_image'])\n",
        "y_train = data_train['label_binary']\n",
        "label_mapping = {'no corrosion': 0, 'corrosion': 1}\n",
        "y_train = y_train.map(label_mapping)\n",
        "\n",
        "X_test = data_test.drop(columns=['label_binary', 'label_multi', 'n_image'])\n",
        "y_test = data_test['label_binary']\n",
        "y_test = y_test.map(label_mapping)\n",
        "\n",
        "# Initialize and train the LightGBM model\n",
        "model = lgbm(\n",
        "    objective='binary',  # Binary classification\n",
        "    boosting_type='goss',  # Boosting type\n",
        "    top_rate=top_rate,             # Top rate\n",
        "    seed=42,                       # For reproducibility\n",
        "    learning_rate=learning_rate,\n",
        "    other_rate=other_rate,\n",
        "    num_leaves=num_leaves,\n",
        "    verbose=-1\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Measure execution time\n",
        "t0 = time.time()\n",
        "y_pred = model.predict(X_test)\n",
        "t1 = time.time()\n",
        "\n",
        "# Print model details and metrics\n",
        "print(f'Model with {num_leaves} leaves, {learning_rate} learning rate, {top_rate} top rate, and {other_rate} other rate:')\n",
        "labels = (1, 0)\n",
        "cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
        "print(f'Confusion matrix: \\n {cm}')\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy}')\n",
        "recall = recall_score(y_test, y_pred, average='binary')\n",
        "recall = round(recall, 3)\n",
        "print(f'Recall: {recall}')\n",
        "specificity = recall_score(y_test, y_pred, average='binary', pos_label=0)\n",
        "specificity = round(specificity, 3)\n",
        "print(f'Specificity: {specificity}')\n",
        "precision = precision_score(y_test, y_pred, average='binary')\n",
        "precision = round(precision, 3)\n",
        "print(f'Precision: {precision}')\n",
        "f1 = f1_score(y_test, y_pred, average='binary')\n",
        "f1 = round(f1, 3)\n",
        "print(f'F1: {f1}')\n",
        "time_taken = t1 - t0\n",
        "time_taken = round(time_taken, 3)\n",
        "print(f'Execution time: {time_taken} seconds')\n",
        "print('\\n')\n",
        "\n",
        "# Create a DataFrame to store results\n",
        "columns = ['Model', 'Learning_rate', 'Num_leaves', 'Top_rate', 'Other_rate',\n",
        "           'Accuracy',\n",
        "           'Recall',\n",
        "           'Specificity',\n",
        "           'Precision',\n",
        "           'F1',\n",
        "           'Time']\n",
        "df_results = pd.DataFrame(columns=columns)\n",
        "\n",
        "# Append results to DataFrame\n",
        "result_lgbm = {'Model': 'LightGBM', 'Accuracy': accuracy, 'Top_rate': top_rate, 'Other_rate': other_rate, 'Learning_rate': learning_rate, 'Num_leaves': num_leaves,\n",
        "               'Recall': recall, 'Specificity': specificity, 'Precision': precision, 'F1': f1,\n",
        "               'Time': time_taken}\n",
        "df_results = pd.concat([df_results, pd.DataFrame([result_lgbm])], ignore_index=True)\n",
        "print(df_results)\n"
      ],
      "metadata": {
        "id": "_q1n_cWDsFz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wR2TvuDtsFz4"
      },
      "source": [
        "## KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SujMQl2EsFz4"
      },
      "outputs": [],
      "source": [
        "# Define the columns for the results DataFrame\n",
        "columns = ['Model', 'N_neighbors',  'Distance', 'Accuracy',\n",
        "           'Recall',\n",
        "           'Specificity',\n",
        "           'Precision',\n",
        "           'F1',\n",
        "           'Time']\n",
        "df_results = pd.DataFrame(columns=columns)\n",
        "\n",
        "# List of neighbors to test\n",
        "N_neighbors = [1, 3, 5, 11, 21, 41, 61]\n",
        "\n",
        "# List of distance metrics to use\n",
        "distance_metrics = ['euclidean', 'manhattan']\n",
        "\n",
        "# Iterate over each number of neighbors and distance metric\n",
        "for i in N_neighbors:\n",
        "    for d in distance_metrics:\n",
        "        # Lists to store results across multiple iterations\n",
        "        recalls = []\n",
        "        specificities = []\n",
        "        precisions = []\n",
        "        f1s = []\n",
        "        accuracies = []\n",
        "        times = []\n",
        "\n",
        "        # Perform cross-validation by iterating over each fold\n",
        "        for j in range(5):\n",
        "\n",
        "            # Select the test dataset for this iteration\n",
        "            d_test = pd.concat([D[j]])\n",
        "            y_test = d_test['label_binary']\n",
        "            X_test = d_test.drop(columns=['label_binary', 'n_image', 'label_multi'])\n",
        "\n",
        "            # Select the training datasets for this iteration\n",
        "            d_train = pd.concat([D[k] for k in range(5) if k != j], ignore_index=True)\n",
        "            y_train = d_train['label_binary']\n",
        "            X_train = d_train.drop(columns=['label_binary', 'n_image', 'label_multi'])\n",
        "\n",
        "            # Scale the data using StandardScaler\n",
        "            scaler = StandardScaler()\n",
        "            X_train = scaler.fit_transform(X_train)\n",
        "            X_test = scaler.transform(X_test)\n",
        "\n",
        "            # Initialize and fit the KNeighborsClassifier model\n",
        "            model = KNeighborsClassifier(n_neighbors=i, metric=d)\n",
        "            model.fit(X_train, y_train)\n",
        "\n",
        "            # Measure the time taken to make predictions\n",
        "            t0 = time.time()\n",
        "            y_pred = model.predict(X_test)\n",
        "            t1 = time.time()\n",
        "\n",
        "            # Print model details\n",
        "            print(f'Model with {i} neighbors using {d} distance:')\n",
        "            labels = ('corrosion', 'no corrosion')\n",
        "            cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
        "            print(f'Confusion matrix for {i} neighbors:\\n {cm}')\n",
        "\n",
        "            # Calculate and store accuracy\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "            accuracies.append(accuracy)\n",
        "\n",
        "            # Calculate and store recall\n",
        "            recall = recall_score(y_test, y_pred, average='binary', pos_label='corrosion')\n",
        "            recall = round(recall, 3)\n",
        "            print(f'Recall for {i} neighbors: {recall}')\n",
        "            recalls.append(recall)\n",
        "\n",
        "            # Calculate and store specificity\n",
        "            specificity = recall_score(y_test, y_pred, average='binary', pos_label='no corrosion')\n",
        "            specificity = round(specificity, 3)\n",
        "            print(f'Specificity: {specificity}')\n",
        "            specificities.append(specificity)\n",
        "\n",
        "            # Calculate and store precision\n",
        "            precision = precision_score(y_test, y_pred, average='binary', pos_label='corrosion')\n",
        "            precision = round(precision, 3)\n",
        "            precisions.append(precision)\n",
        "\n",
        "            # Calculate and store F1 score\n",
        "            f1 = f1_score(y_test, y_pred, average='binary', pos_label='corrosion')\n",
        "            f1 = round(f1, 3)\n",
        "            f1s.append(f1)\n",
        "\n",
        "            # Calculate and store time taken\n",
        "            time_taken = t1 - t0\n",
        "            time_taken = round(time_taken, 3)\n",
        "            times.append(time_taken)\n",
        "            print(f'Execution time: {time_taken} seconds')\n",
        "            print('\\n')\n",
        "\n",
        "        # Calculate mean metrics across all iterations\n",
        "        recall_mean = np.mean(recalls)\n",
        "        specificity_mean = np.mean(specificities)\n",
        "        precision_mean = np.mean(precisions)\n",
        "        f1_mean = np.mean(f1s)\n",
        "        accuracy_mean = np.mean(accuracies)\n",
        "        time_mean = np.mean(times)\n",
        "\n",
        "        # Create a dictionary to store the results for this model configuration\n",
        "        result_i = {\n",
        "            'Model': 'KNN',\n",
        "            'Accuracy': accuracy_mean,\n",
        "            'N_neighbors': i,\n",
        "            'Distance': d,\n",
        "            'Recall': recall_mean,\n",
        "            'Specificity': specificity_mean,\n",
        "            'Precision': precision_mean,\n",
        "            'F1': f1_mean,\n",
        "            'Time': time_mean\n",
        "        }\n",
        "\n",
        "        # Append the results to the DataFrame\n",
        "        df_results = pd.concat([df_results, pd.DataFrame([result_i])], ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zAA4x-lsFz5"
      },
      "outputs": [],
      "source": [
        "K = 61                        # Number of neighbors\n",
        "DISTANCE_METRIC = 'manhattan' # Distance metric for KNN\n",
        "LABELS = ('corrosion', 'no corrosion')  # Class labels\n",
        "\n",
        "# ========================\n",
        "# Data Preparation\n",
        "# ========================\n",
        "# Prepare training data\n",
        "X_train = data_train.drop(columns=['label_binary', 'n_image', 'label_multi'])\n",
        "y_train = data_train['label_binary']\n",
        "\n",
        "# Prepare test data\n",
        "X_test = data_test.drop(columns=['label_binary', 'n_image', 'label_multi'])\n",
        "y_test = data_test['label_binary']\n",
        "\n",
        "# ========================\n",
        "# Feature Scaling\n",
        "# ========================\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# ========================\n",
        "# Model Training\n",
        "# ========================\n",
        "model = KNeighborsClassifier(\n",
        "    n_neighbors=K,\n",
        "    metric=DISTANCE_METRIC\n",
        ")\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# ========================\n",
        "# Model Evaluation\n",
        "# ========================\n",
        "# Time prediction process\n",
        "start_time = time.time()\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "execution_time = round(time.time() - start_time, 3)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "cm = confusion_matrix(y_test, y_pred, labels=LABELS)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "recall = round(recall_score(y_test, y_pred, average='binary', pos_label='corrosion'), 3)\n",
        "specificity = round(recall_score(y_test, y_pred, average='binary', pos_label='no corrosion'), 3)\n",
        "precision = round(precision_score(y_test, y_pred, average='binary', pos_label='corrosion'), 3)\n",
        "f1 = round(f1_score(y_test, y_pred, average='binary', pos_label='corrosion'), 3)\n",
        "\n",
        "# ========================\n",
        "# Results Display\n",
        "# ========================\n",
        "print(f'\\nModel with {K} neighbors using {DISTANCE_METRIC} distance:')\n",
        "print(f'Confusion Matrix:\\n{cm}')\n",
        "print(f'Accuracy: {accuracy:.3f}')\n",
        "print(f'Recall (Corrosion Detection): {recall}')\n",
        "print(f'Specificity (No Corrosion): {specificity}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'F1 Score: {f1}')\n",
        "print(f'Execution Time: {execution_time} seconds')\n",
        "\n",
        "# ========================\n",
        "# Results Storage\n",
        "# ========================\n",
        "results_columns = ['Model', 'N_neighbors', 'Distance', 'Accuracy',\n",
        "                  'Recall', 'Specificity', 'Precision', 'F1', 'Time']\n",
        "\n",
        "results_data = {\n",
        "    'Model': 'KNN',\n",
        "    'N_neighbors': K,\n",
        "    'Distance': DISTANCE_METRIC,\n",
        "    'Accuracy': accuracy,\n",
        "    'Recall': recall,\n",
        "    'Specificity': specificity,\n",
        "    'Precision': precision,\n",
        "    'F1': f1,\n",
        "    'Time': execution_time\n",
        "}\n",
        "\n",
        "df_final_results = pd.DataFrame([results_data])\n",
        "print('\\nFinal Results DataFrame:')\n",
        "print(df_final_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kzr_OXibsFz6"
      },
      "source": [
        "## Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define results DataFrame structure\n",
        "results_columns = ['Model', 'Accuracy', 'Recall', 'Specificity',\n",
        "                  'Precision', 'F1', 'Time']\n",
        "df_results = pd.DataFrame(columns=results_columns)\n",
        "\n",
        "# Configuration parameters\n",
        "VIF_THRESHOLD = 10\n",
        "PVAL_THRESHOLD = 0.01\n",
        "MAX_ITERATIONS = 100\n",
        "\n",
        "def perform_feature_selection(X_train, y_train):\n",
        "    \"\"\"\n",
        "    Perform iterative feature selection using p-values and VIF analysis.\n",
        "\n",
        "    Args:\n",
        "        X_train: Training features DataFrame\n",
        "        y_train: Training labels Series\n",
        "\n",
        "    Returns:\n",
        "        Tuple: (Final model, List of selected features)\n",
        "    \"\"\"\n",
        "    current_features = X_train.columns.tolist()\n",
        "    removed_features = []\n",
        "\n",
        "    # First phase: Remove features with high p-values\n",
        "    while True:\n",
        "        model = sm.Logit(y_train, X_train[current_features]).fit(\n",
        "            disp=0,\n",
        "            maxiter=MAX_ITERATIONS\n",
        "        )\n",
        "\n",
        "        # Get highest p-value feature\n",
        "        p_values = model.pvalues\n",
        "        max_p_feature = p_values.idxmax()\n",
        "        max_p_value = p_values.max()\n",
        "\n",
        "        if max_p_value < PVAL_THRESHOLD:\n",
        "            break\n",
        "\n",
        "        print(f\"Removing feature: {max_p_feature} (p-value: {max_p_value:.3f})\")\n",
        "        current_features.remove(max_p_feature)\n",
        "\n",
        "    # Second phase: Remove features with high multicollinearity\n",
        "    while True:\n",
        "        # Calculate VIF for remaining features\n",
        "        vif_data = pd.DataFrame()\n",
        "        vif_data['Feature'] = current_features\n",
        "        vif_data['VIF'] = [variance_inflation_factor(X_train[current_features].values, i)\n",
        "                          for i in range(len(current_features))]\n",
        "\n",
        "        max_vif = vif_data['VIF'].max()\n",
        "        max_vif_feature = vif_data.loc[vif_data['VIF'].idxmax(), 'Feature']\n",
        "\n",
        "        if max_vif < VIF_THRESHOLD:\n",
        "            print(\"All VIF values below threshold\")\n",
        "            break\n",
        "\n",
        "        print(f\"Removing feature: {max_vif_feature} (VIF: {max_vif:.1f})\")\n",
        "        current_features.remove(max_vif_feature)\n",
        "\n",
        "    return model, current_features\n",
        "\n",
        "# Main cross-validation loop\n",
        "for fold_idx in tqdm(range(5), desc=\"Processing folds\"):\n",
        "    # Split data into train/test sets\n",
        "    test_data = pd.concat([D[fold_idx]])\n",
        "    train_data = pd.concat([d for i, d in enumerate(D) if i != fold_idx])\n",
        "\n",
        "    # Prepare datasets\n",
        "    y_test = test_data['label_binary']\n",
        "    X_test = test_data.drop(columns=['label_binary', 'n_image', 'label_multi'])\n",
        "\n",
        "    y_train = train_data['label_binary']\n",
        "    X_train = train_data.drop(columns=['label_binary', 'n_image', 'label_multi'])\n",
        "\n",
        "    # Encode labels to numerical values\n",
        "    label_mapping = {'no corrosion': 0, 'corrosion': 1}\n",
        "    y_train = y_train.map(label_mapping)\n",
        "    y_test = y_test.map(label_mapping)\n",
        "\n",
        "    # Standardize features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Convert back to DataFrames with original column names\n",
        "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
        "\n",
        "    # Feature selection process\n",
        "    final_model, selected_features = perform_feature_selection(X_train_scaled, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    start_time = time.time()\n",
        "    X_test_final = X_test_scaled[selected_features]\n",
        "    y_pred_proba = final_model.predict(X_test_final)\n",
        "    y_pred = (y_pred_proba >= 0.5).astype(int)\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    # Calculate metrics\n",
        "    cm = confusion_matrix(y_test, y_pred, labels=[1, 0])\n",
        "    print(f\"\\nConfusion matrix for fold {fold_idx+1}:\\n{cm}\")\n",
        "\n",
        "    fold_metrics = {\n",
        "        'accuracy': accuracy_score(y_test, y_pred),\n",
        "        'recall': recall_score(y_test, y_pred, pos_label=1),\n",
        "        'specificity': recall_score(y_test, y_pred, pos_label=0),\n",
        "        'precision': precision_score(y_test, y_pred, pos_label=1),\n",
        "        'f1': f1_score(y_test, y_pred, pos_label=1),\n",
        "        'time': elapsed_time\n",
        "    }\n",
        "\n",
        "    # Store metrics\n",
        "    df_results = pd.concat([\n",
        "        df_results,\n",
        "        pd.DataFrame([{\n",
        "            'Model': 'Logistic Regression',\n",
        "            'Accuracy': fold_metrics['accuracy'],\n",
        "            'Recall': fold_metrics['recall'],\n",
        "            'Specificity': fold_metrics['specificity'],\n",
        "            'Precision': fold_metrics['precision'],\n",
        "            'F1': fold_metrics['f1'],\n",
        "            'Time': fold_metrics['time']\n",
        "        }])\n",
        "    ], ignore_index=True)\n",
        "\n",
        "# Calculate mean metrics across all folds\n",
        "mean_results = {\n",
        "    'Model': 'Logistic Regression',\n",
        "    'Accuracy': df_results['Accuracy'].mean(),\n",
        "    'Recall': df_results['Recall'].mean(),\n",
        "    'Specificity': df_results['Specificity'].mean(),\n",
        "    'Precision': df_results['Precision'].mean(),\n",
        "    'F1': df_results['F1'].mean(),\n",
        "    'Time': df_results['Time'].mean()\n",
        "}\n",
        "\n",
        "# Add mean results to DataFrame\n",
        "df_results = pd.concat([\n",
        "    df_results,\n",
        "    pd.DataFrame([mean_results])\n",
        "], ignore_index=True)\n",
        "\n",
        "print(\"\\nFinal Results:\")\n",
        "print(df_results.round(3))"
      ],
      "metadata": {
        "id": "W7f6LwYhsFz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhrr03VJsFz8"
      },
      "source": [
        "## SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a79lj_j5sFz8"
      },
      "outputs": [],
      "source": [
        "# Define the columns for the results DataFrame\n",
        "columns = ['Model', 'C', 'Kernel', 'Accuracy', 'Recall', 'Specificity', 'Precision', 'F1', 'Time']\n",
        "df_results = pd.DataFrame(columns=columns)\n",
        "\n",
        "# Hyperparameter values for SVM\n",
        "C_values = [0.001, 0.01, 0.1, 1]  # Regularization parameter values\n",
        "kernels = ['linear', 'rbf']  # Kernel types to test\n",
        "\n",
        "# Iterate over each combination of C and kernel\n",
        "for c in C_values:\n",
        "    for kernel in kernels:\n",
        "        # Initialize lists to store metrics across folds\n",
        "        recalls = []\n",
        "        specificities = []\n",
        "        precisions = []\n",
        "        f1s = []\n",
        "        accuracies = []\n",
        "        times = []\n",
        "\n",
        "        # Perform 5-fold cross-validation\n",
        "        for fold in range(5):\n",
        "            # Split data into training and testing sets for this fold\n",
        "            test_data = pd.concat([D[fold]])\n",
        "            y_test = test_data['label_binary']\n",
        "            X_test = test_data.drop(columns=['label_binary', 'n_image', 'label_multi'])\n",
        "\n",
        "            train_data = pd.concat([D[i] for i in range(5) if i != fold], ignore_index=True)\n",
        "            y_train = train_data['label_binary']\n",
        "            X_train = train_data.drop(columns=['label_binary', 'n_image', 'label_multi'])\n",
        "\n",
        "            # Scale the features using StandardScaler\n",
        "            scaler = StandardScaler()\n",
        "            X_train_scaled = scaler.fit_transform(X_train)\n",
        "            X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "            # Train the SVM model with the current hyperparameters\n",
        "            model = SVC(C=c, kernel=kernel, verbose=False)\n",
        "            model.fit(X_train_scaled, y_train)\n",
        "\n",
        "            # Measure prediction time\n",
        "            t0 = time.time()\n",
        "            y_pred = model.predict(X_test_scaled)\n",
        "            t1 = time.time()\n",
        "\n",
        "            # Print model details and confusion matrix\n",
        "            print(f'Model with C={c} and kernel={kernel}:')\n",
        "            labels = ('corrosion', 'no corrosion')\n",
        "            cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
        "            print(f'Confusion matrix for C={c}:\\n{cm}')\n",
        "\n",
        "            # Calculate metrics and store them\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "            accuracies.append(accuracy)\n",
        "\n",
        "            recall = recall_score(y_test, y_pred, average='binary', pos_label='corrosion')\n",
        "            recall = round(recall, 3)\n",
        "            print(f'Recall for C={c}: {recall}')\n",
        "            recalls.append(recall)\n",
        "\n",
        "            specificity = recall_score(y_test, y_pred, average='binary', pos_label='no corrosion')\n",
        "            specificity = round(specificity, 3)\n",
        "            specificities.append(specificity)\n",
        "            print(f'Specificity: {specificity}')\n",
        "\n",
        "            precision = precision_score(y_test, y_pred, average='binary', pos_label='corrosion')\n",
        "            precision = round(precision, 3)\n",
        "            precisions.append(precision)\n",
        "\n",
        "            f1 = f1_score(y_test, y_pred, average='binary', pos_label='corrosion')\n",
        "            f1 = round(f1, 3)\n",
        "            f1s.append(f1)\n",
        "\n",
        "            time_taken = t1 - t0\n",
        "            time_taken = round(time_taken, 3)\n",
        "            times.append(time_taken)\n",
        "            print(f'Execution time: {time_taken} seconds\\n')\n",
        "\n",
        "        # Calculate mean metrics across all folds\n",
        "        mean_recall = np.mean(recalls)\n",
        "        mean_specificity = np.mean(specificities)\n",
        "        mean_precision = np.mean(precisions)\n",
        "        mean_f1 = np.mean(f1s)\n",
        "        mean_accuracy = np.mean(accuracies)\n",
        "        mean_time = np.mean(times)\n",
        "\n",
        "        # Create a dictionary to store the results for this configuration\n",
        "        result_row = {\n",
        "            'Model': 'SVM',\n",
        "            'C': c,\n",
        "            'Kernel': kernel,\n",
        "            'Accuracy': mean_accuracy,\n",
        "            'Recall': mean_recall,\n",
        "            'Specificity': mean_specificity,\n",
        "            'Precision': mean_precision,\n",
        "            'F1': mean_f1,\n",
        "            'Time': mean_time\n",
        "        }\n",
        "\n",
        "        # Append the results to the DataFrame\n",
        "        df_results = pd.concat([df_results, pd.DataFrame([result_row])], ignore_index=True)\n",
        "\n",
        "# Display final results summary\n",
        "print(\"\\nFinal Results:\")\n",
        "print(df_results.round(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLSOV1ZjsFz9"
      },
      "outputs": [],
      "source": [
        "C_VALUE = 0.1                # Regularization parameter\n",
        "KERNEL_TYPE = 'rbf'          # Kernel type for SVM\n",
        "LABEL_MAPPING = {            # Label encoding dictionary\n",
        "    'no corrosion': 0,\n",
        "    'corrosion': 1\n",
        "}\n",
        "\n",
        "# ========================\n",
        "# Data Preparation\n",
        "# ========================\n",
        "# Separate features and labels for training data\n",
        "X_train = data_train.drop(columns=['label_binary', 'label_multi', 'n_image'])\n",
        "y_train = data_train['label_binary']\n",
        "\n",
        "# Separate features and labels for test data\n",
        "X_test = data_test.drop(columns=['label_binary', 'label_multi', 'n_image'])\n",
        "y_test = data_test['label_binary']\n",
        "\n",
        "# ========================\n",
        "# Feature Scaling\n",
        "# ========================\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# ========================\n",
        "# Label Encoding\n",
        "# ========================\n",
        "y_train_encoded = y_train.map(LABEL_MAPPING)\n",
        "y_test_encoded = y_test.map(LABEL_MAPPING)\n",
        "\n",
        "# ========================\n",
        "# Model Training\n",
        "# ========================\n",
        "model = SVC(C=C_VALUE, kernel=KERNEL_TYPE, verbose=True)\n",
        "model.fit(X_train_scaled, y_train_encoded)\n",
        "\n",
        "# ========================\n",
        "# Model Evaluation\n",
        "# ========================\n",
        "# Time prediction only\n",
        "start_time = time.time()\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "execution_time = round(time.time() - start_time, 3)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "cm = confusion_matrix(y_test_encoded, y_pred, labels=[1, 0])\n",
        "accuracy = accuracy_score(y_test_encoded, y_pred)\n",
        "recall = round(recall_score(y_test_encoded, y_pred, pos_label=1), 3)\n",
        "specificity = round(recall_score(y_test_encoded, y_pred, pos_label=0), 3)\n",
        "precision = round(precision_score(y_test_encoded, y_pred, pos_label=1), 3)\n",
        "f1 = round(f1_score(y_test_encoded, y_pred, pos_label=1), 3)\n",
        "\n",
        "# ========================\n",
        "# Results Display\n",
        "# ========================\n",
        "print(f'\\nModel with C={C_VALUE} and {KERNEL_TYPE} kernel:')\n",
        "print(f'Confusion Matrix:\\n{cm}')\n",
        "print(f'Accuracy: {accuracy:.3f}')\n",
        "print(f'Recall (Corrosion): {recall}')\n",
        "print(f'Specificity (No Corrosion): {specificity}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'F1 Score: {f1}')\n",
        "print(f'Execution Time: {execution_time} seconds')\n",
        "\n",
        "# ========================\n",
        "# Results Storage\n",
        "# ========================\n",
        "results_columns = ['Model', 'C', 'Kernel', 'Accuracy',\n",
        "                  'Recall', 'Specificity', 'Precision', 'F1', 'Time']\n",
        "results_data = {\n",
        "    'Model': 'SVM',\n",
        "    'C': C_VALUE,\n",
        "    'Kernel': KERNEL_TYPE,\n",
        "    'Accuracy': accuracy,\n",
        "    'Recall': recall,\n",
        "    'Specificity': specificity,\n",
        "    'Precision': precision,\n",
        "    'F1': f1,\n",
        "    'Time': execution_time\n",
        "}\n",
        "\n",
        "df_results = pd.DataFrame([results_data])\n",
        "print('\\nResults DataFrame:')\n",
        "print(df_results)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}